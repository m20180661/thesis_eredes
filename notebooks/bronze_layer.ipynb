{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b528dc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports in the first cell\n",
    "    import requests\n",
    "    import pandas as pd\n",
    "    from pyspark.sql import SparkSession\n",
    "    from io import BytesIO\n",
    "    import csv\n",
    "    import os\n",
    "    from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f55d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-09T08:15:36.6573846Z\",\"execution_start_time\":\"2025-06-09T08:15:36.3817945Z\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\",\"parent_msg_id\":\"012c6cee-9e86-4015-8c5d-dd1dbe894ab4\",\"queued_time\":\"2025-06-09T08:15:36.3805715Z\",\"session_id\":\"a6c537fe-2fee-46be-bd44-ef643e81d2ac\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"finished\",\"statement_id\":9,\"statement_ids\":[9]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5884726",
   "metadata": {},
   "outputs": [],
   "source": [
    "Energy Injected into the Distribution Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b63d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset ID: energia-injetada-na-rede-de-distribuicao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a58916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "    from datetime import datetime\n",
    "    from pyspark.sql import SparkSession\n",
    "    import pandas as pd\n",
    "    from io import BytesIO\n",
    "    import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5448555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset name and ABFS path in the Lakehouse\n",
    "    DATASET_NAME = \"energia-injetada-na-rede-de-distribuicao\"\n",
    "    ABFS_DIR = \"abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/energia_injetada_na_rede_de_distribuicao\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b277b50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Download full CSV via API\n",
    "    def download_dataset():\n",
    "        url = f\"https://e-redes.opendatasoft.com/api/explore/v2.1/catalog/datasets/{DATASET_NAME}/exports/csv\"\n",
    "        print(f\"üîÑ Downloading full CSV from '{DATASET_NAME}' ...\")\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"‚ùå Failed to export: {response.status_code}\")\n",
    "        return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436dcd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Try to detect the delimiter, fallback to semicolon\n",
    "    def detect_separator(sample_text):\n",
    "        sniffer = csv.Sniffer()\n",
    "        try:\n",
    "            dialect = sniffer.sniff(sample_text)\n",
    "            return dialect.delimiter\n",
    "        except csv.Error:\n",
    "            print(\"‚ö†Ô∏è Could not auto-detect delimiter. Falling back to ';'\")\n",
    "            return \";\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb545da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Load using pandas, convert to Spark, write to ABFS\n",
    "    def save_and_upload_to_abfs():\n",
    "        csv_bytes = download_dataset()\n",
    "        sample = csv_bytes.decode('utf-8')[:1000]\n",
    "        sep = detect_separator(sample)\n",
    "        print(f\"‚ÑπÔ∏è Detected separator: '{sep}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c373d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read into pandas with fallback separator\n",
    "        pdf = pd.read_csv(BytesIO(csv_bytes), sep=sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407cb09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Spark DataFrame\n",
    "        df = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4023ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to ABFS with overwrite mode\n",
    "        df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(ABFS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f12f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"‚úÖ File written to: {ABFS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df62a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute function\n",
    "    save_and_upload_to_abfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5e5187",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-09T08:16:10.1336401Z\",\"execution_start_time\":\"2025-06-09T08:15:36.6594525Z\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\",\"parent_msg_id\":\"be6b1ea2-81b9-4d70-aa86-d000ec9e6443\",\"queued_time\":\"2025-06-09T08:15:36.5143515Z\",\"session_id\":\"a6c537fe-2fee-46be-bd44-ef643e81d2ac\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"finished\",\"statement_id\":10,\"statement_ids\":[10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd4a7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "üîÑ Downloading full CSV from 'energia-injetada-na-rede-de-distribuicao' ...\n",
    "    ‚ö†Ô∏è Could not auto-detect delimiter. Falling back to ';'\n",
    "    ‚ÑπÔ∏è Detected separator: ';'\n",
    "    ‚úÖ File written to: abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/energia_injetada_na_rede_de_distribuicao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b740f859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f708bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Primeiro comando: Energia Injetada na Rede de Distribui√ß√£o ---\n",
    "    # ... (o teu c√≥digo de extra√ß√£o do primeiro dataset aqui) ...\n",
    "    print(\"[INFO] Finished **Energy Injected into the Distribution Network ** **Dataset ID:** energia-injetada-na-rede-de-distribuicao. Pausing before next extraction...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429ff3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pausa entre comandos ---\n",
    "    time_to_sleep = 60  # segundos (podes aumentar se quiseres mais seguro, ex: 120)\n",
    "    for remaining in range(time_to_sleep, 0, -1):\n",
    "        print(f\"Sleeping for {remaining} seconds...\", end=\"\\r\")\n",
    "        time.sleep(1)\n",
    "    print(\"\\n[INFO] Sleep complete. Proceeding to next extraction.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bef3b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-09T08:17:11.9686712Z\",\"execution_start_time\":\"2025-06-09T08:16:10.1361634Z\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\",\"parent_msg_id\":\"1370c6d5-f94a-4978-b879-efc391e60e2d\",\"queued_time\":\"2025-06-09T08:15:36.8588824Z\",\"session_id\":\"a6c537fe-2fee-46be-bd44-ef643e81d2ac\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"finished\",\"statement_id\":11,\"statement_ids\":[11]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a5f7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "[INFO] Finished **Energy Injected into the Distribution Network ** **Dataset ID:** energia-injetada-na-rede-de-distribuicao. Pausing before next extraction...\n",
    "    Sleeping for 1 seconds....\n",
    "    [INFO] Sleep complete. Proceeding to next extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f309cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Reception Capacity in the National Distribution Network (RND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12cfbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset ID: capacidade_rececao_rnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4078800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "    from datetime import datetime\n",
    "    from pyspark.sql import SparkSession\n",
    "    import pandas as pd\n",
    "    from io import BytesIO\n",
    "    import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f35e933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset name and ABFS path in the Lakehouse\n",
    "    DATASET_NAME = \"capacidade-rececao-rnd\"\n",
    "    ABFS_DIR = \"abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/capacidade_rececao_rnd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4731ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Download full CSV via API\n",
    "    def download_dataset():\n",
    "        url = f\"https://e-redes.opendatasoft.com/api/explore/v2.1/catalog/datasets/{DATASET_NAME}/exports/csv\"\n",
    "        print(f\"üîÑ Downloading full CSV from '{DATASET_NAME}' ...\")\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"‚ùå Failed to export: {response.status_code}\")\n",
    "        return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24390eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Try to detect the delimiter, fallback to semicolon\n",
    "    def detect_separator(sample_text):\n",
    "        sniffer = csv.Sniffer()\n",
    "        try:\n",
    "            dialect = sniffer.sniff(sample_text)\n",
    "            return dialect.delimiter\n",
    "        except csv.Error:\n",
    "            print(\"‚ö†Ô∏è Could not auto-detect delimiter. Falling back to ';'\")\n",
    "            return \";\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c644a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Load using pandas, convert to Spark, write to ABFS\n",
    "    def save_and_upload_to_abfs():\n",
    "        csv_bytes = download_dataset()\n",
    "        sample = csv_bytes.decode('utf-8')[:1000]\n",
    "        sep = detect_separator(sample)\n",
    "        print(f\"‚ÑπÔ∏è Detected separator: '{sep}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5142f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read into pandas with fallback separator\n",
    "        pdf = pd.read_csv(BytesIO(csv_bytes), sep=sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a6f022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Spark DataFrame\n",
    "        df = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a64475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to ABFS with overwrite mode\n",
    "        df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(ABFS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d4da3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"‚úÖ File written to: {ABFS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c943d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute function\n",
    "    save_and_upload_to_abfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baca6106",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-09T08:20:48.0127279Z\",\"execution_start_time\":\"2025-06-09T08:20:45.552855Z\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\",\"parent_msg_id\":\"694411ec-477e-4571-8c52-662ca874d65c\",\"queued_time\":\"2025-06-09T08:20:45.5515777Z\",\"session_id\":\"a6c537fe-2fee-46be-bd44-ef643e81d2ac\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"finished\",\"statement_id\":17,\"statement_ids\":[17]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449943d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "üîÑ Downloading full CSV from 'capacidade-rececao-rnd' ...\n",
    "    ‚ÑπÔ∏è Detected separator: ';'\n",
    "    ‚úÖ File written to: abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/capacidade-rececao-rnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89201be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81982e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Primeiro comando: Energia Injetada na Rede de Distribui√ß√£o ---\n",
    "    # ... (o teu c√≥digo de extra√ß√£o do primeiro dataset aqui) ...\n",
    "    print(\"[INFO] Finished **Reception Capacity in the National Distribution Network (RND)** **Dataset ID:** capacidade-rececao-rnd. Pausing before next extraction...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324d06f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pausa entre comandos ---\n",
    "    time_to_sleep = 60  # segundos (podes aumentar se quiseres mais seguro, ex: 120)\n",
    "    for remaining in range(time_to_sleep, 0, -1):\n",
    "        print(f\"Sleeping for {remaining} seconds...\", end=\"\\r\")\n",
    "        time.sleep(1)\n",
    "    print(\"\\n[INFO] Sleep complete. Proceeding to next extraction.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff82cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-09T08:18:14.8471229Z\",\"execution_start_time\":\"2025-06-09T08:17:14.3293461Z\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\",\"parent_msg_id\":\"80fa1dd7-01cf-44ea-be3c-aa380f25d4ae\",\"queued_time\":\"2025-06-09T08:15:37.8785507Z\",\"session_id\":\"a6c537fe-2fee-46be-bd44-ef643e81d2ac\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"finished\",\"statement_id\":13,\"statement_ids\":[13]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f964f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "[INFO] Finished **Reception Capacity in the National Distribution Network (RND)** **Dataset ID:** capacidade-rececao-rnd. Pausing before next extraction...\n",
    "    Sleeping for 1 seconds....\n",
    "    [INFO] Sleep complete. Proceeding to next extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac459d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "Total National Consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd93fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset ID: consumo_total_nacional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e8066d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "    from datetime import datetime\n",
    "    from pyspark.sql import SparkSession\n",
    "    import pandas as pd\n",
    "    from io import BytesIO\n",
    "    import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ed7d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset name and ABFS path in the Lakehouse\n",
    "    DATASET_NAME = \"consumo-total-nacional\"\n",
    "    ABFS_DIR = \"abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/consumo_total_nacional\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0035f095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Download full CSV via API\n",
    "    def download_dataset():\n",
    "        url = f\"https://e-redes.opendatasoft.com/api/explore/v2.1/catalog/datasets/{DATASET_NAME}/exports/csv\"\n",
    "        print(f\"üîÑ Downloading full CSV from '{DATASET_NAME}' ...\")\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"‚ùå Failed to export: {response.status_code}\")\n",
    "        return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9994ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Try to detect the delimiter, fallback to semicolon\n",
    "    def detect_separator(sample_text):\n",
    "        sniffer = csv.Sniffer()\n",
    "        try:\n",
    "            dialect = sniffer.sniff(sample_text)\n",
    "            return dialect.delimiter\n",
    "        except csv.Error:\n",
    "            print(\"‚ö†Ô∏è Could not auto-detect delimiter. Falling back to ';'\")\n",
    "            return \";\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f497b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Load using pandas, convert to Spark, write to ABFS\n",
    "    def save_and_upload_to_abfs():\n",
    "        csv_bytes = download_dataset()\n",
    "        sample = csv_bytes.decode('utf-8')[:1000]\n",
    "        sep = detect_separator(sample)\n",
    "        print(f\"‚ÑπÔ∏è Detected separator: '{sep}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5077b239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read into pandas with fallback separator\n",
    "        pdf = pd.read_csv(BytesIO(csv_bytes), sep=sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd12f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Spark DataFrame\n",
    "        df = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ec45cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to ABFS with overwrite mode\n",
    "        df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(ABFS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a4d699",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"‚úÖ File written to: {ABFS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe4e015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute function\n",
    "    save_and_upload_to_abfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8a6a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-09T08:20:02.6195753Z\",\"execution_start_time\":\"2025-06-09T08:19:36.0817594Z\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\",\"parent_msg_id\":\"9c387cdd-abbf-4675-b027-7499341fa8fb\",\"queued_time\":\"2025-06-09T08:19:36.0804936Z\",\"session_id\":\"a6c537fe-2fee-46be-bd44-ef643e81d2ac\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"finished\",\"statement_id\":15,\"statement_ids\":[15]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e036608",
   "metadata": {},
   "outputs": [],
   "source": [
    "üîÑ Downloading full CSV from 'consumo-total-nacional' ...\n",
    "    ‚ö†Ô∏è Could not auto-detect delimiter. Falling back to ';'\n",
    "    ‚ÑπÔ∏è Detected separator: ';'\n",
    "    ‚úÖ File written to: abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/consumo_total_nacional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68d2292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661a5051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Primeiro comando: Energia Injetada na Rede de Distribui√ß√£o ---\n",
    "    # ... (o teu c√≥digo de extra√ß√£o do primeiro dataset aqui) ...\n",
    "    print(\"[INFO] Finished **Total National Consumption** **Dataset ID:** consumo-total-nacional. Pausing before next extraction...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5ad2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pausa entre comandos ---\n",
    "    time_to_sleep = 60  # segundos (podes aumentar se quiseres mais seguro, ex: 120)\n",
    "    for remaining in range(time_to_sleep, 0, -1):\n",
    "        print(f\"Sleeping for {remaining} seconds...\", end=\"\\r\")\n",
    "        time.sleep(1)\n",
    "    print(\"\\n[INFO] Sleep complete. Proceeding to next extraction.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755f5a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-09T08:18:40.5382991Z\",\"execution_start_time\":null,\"livy_statement_state\":null,\"normalized_state\":\"cancelled\",\"parent_msg_id\":\"36e8416a-c00e-441d-9fd5-16d3dede97ff\",\"queued_time\":\"2025-06-09T08:15:38.4679426Z\",\"session_id\":\"a6c537fe-2fee-46be-bd44-ef643e81d2ac\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"cancelled\",\"statement_id\":-1,\"statement_ids\":null}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5b94c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Total National Produced Energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b448c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset ID: energia_produzida_total_nacional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2358ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "    from datetime import datetime\n",
    "    from pyspark.sql import SparkSession\n",
    "    import pandas as pd\n",
    "    from io import BytesIO\n",
    "    import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bfbccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset name and ABFS path in the Lakehouse\n",
    "    DATASET_NAME = \"energia-produzida-total-nacional\"\n",
    "    ABFS_DIR = \"abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/energia_produzida_total_nacional\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878ada4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Download full CSV via API\n",
    "    def download_dataset():\n",
    "        url = f\"https://e-redes.opendatasoft.com/api/explore/v2.1/catalog/datasets/{DATASET_NAME}/exports/csv\"\n",
    "        print(f\"üîÑ Downloading full CSV from '{DATASET_NAME}' ...\")\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"‚ùå Failed to export: {response.status_code}\")\n",
    "        return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b1ef77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Try to detect the delimiter, fallback to semicolon\n",
    "    def detect_separator(sample_text):\n",
    "        sniffer = csv.Sniffer()\n",
    "        try:\n",
    "            dialect = sniffer.sniff(sample_text)\n",
    "            return dialect.delimiter\n",
    "        except csv.Error:\n",
    "            print(\"‚ö†Ô∏è Could not auto-detect delimiter. Falling back to ';'\")\n",
    "            return \";\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65b0a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Load using pandas, convert to Spark, write to ABFS\n",
    "    def save_and_upload_to_abfs():\n",
    "        csv_bytes = download_dataset()\n",
    "        sample = csv_bytes.decode('utf-8')[:1000]\n",
    "        sep = detect_separator(sample)\n",
    "        print(f\"‚ÑπÔ∏è Detected separator: '{sep}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce40383e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read into pandas with fallback separator\n",
    "        pdf = pd.read_csv(BytesIO(csv_bytes), sep=sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef0a53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Spark DataFrame\n",
    "        df = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9af1a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to ABFS with overwrite mode\n",
    "        df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(ABFS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c56c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"‚úÖ File written to: {ABFS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8c7a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute function\n",
    "    save_and_upload_to_abfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80762e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-09T08:22:39.2093852Z\",\"execution_start_time\":\"2025-06-09T08:22:14.6816607Z\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\",\"parent_msg_id\":\"f913ef2a-668a-4132-ad24-4d0a2c93fa88\",\"queued_time\":\"2025-06-09T08:22:14.6803083Z\",\"session_id\":\"a6c537fe-2fee-46be-bd44-ef643e81d2ac\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"finished\",\"statement_id\":19,\"statement_ids\":[19]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174041b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "üîÑ Downloading full CSV from 'energia-produzida-total-nacional' ...\n",
    "    ‚ÑπÔ∏è Detected separator: ';'\n",
    "    ‚úÖ File written to: abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/energia_produzida_total_nacional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cc32ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c40840d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] Finished 'Total National Consumption' (Dataset ID: consumo-total-nacional). Pausing before next extraction...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc2a2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pause between dataset extraction commands ---\n",
    "    time_to_sleep = 60  # seconds (change if you want, e.g. 120 for extra caution)\n",
    "    for remaining in range(time_to_sleep, 0, -1):\n",
    "        print(f\"Sleeping for {remaining} seconds...\", end=\"\\r\")\n",
    "        time.sleep(1)\n",
    "    print(\"\\n[INFO] Sleep complete. Proceeding to next extraction.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13c5351",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-09T08:18:40.5388935Z\",\"execution_start_time\":null,\"livy_statement_state\":null,\"normalized_state\":\"cancelled\",\"parent_msg_id\":\"79fce5dd-2ea4-4d67-bb95-c3266fe017c9\",\"queued_time\":\"2025-06-09T08:15:38.9318342Z\",\"session_id\":\"a6c537fe-2fee-46be-bd44-ef643e81d2ac\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"cancelled\",\"statement_id\":-1,\"statement_ids\":null}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82917f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "Monthly consumption by municipality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2220db",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset ID: 3_consumos_faturados_por_municipio_ultimos_10anos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9240b7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "    from datetime import datetime\n",
    "    from pyspark.sql import SparkSession\n",
    "    import pandas as pd\n",
    "    from io import BytesIO\n",
    "    import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5201e068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset name and ABFS path in the Lakehouse\n",
    "    DATASET_NAME = \"3-consumos-faturados-por-municipio-ultimos-10-anos\"\n",
    "    ABFS_DIR = \"abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/3_consumos_faturados_por_municipio-ultimos_10_anos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588a5269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Download the full CSV from the API\n",
    "    def download_dataset():\n",
    "        url = f\"https://e-redes.opendatasoft.com/api/explore/v2.1/catalog/datasets/{DATASET_NAME}/exports/csv\"\n",
    "        print(f\"üîÑ Downloading full CSV from '{DATASET_NAME}' ...\")\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"‚ùå Failed to export: {response.status_code}\")\n",
    "        return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d332ce00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Automatically detect CSV delimiter with fallback\n",
    "    def detect_separator(sample_text):\n",
    "        sniffer = csv.Sniffer()\n",
    "        try:\n",
    "            dialect = sniffer.sniff(sample_text)\n",
    "            return dialect.delimiter\n",
    "        except csv.Error:\n",
    "            print(\"‚ö†Ô∏è Could not auto-detect delimiter. Falling back to ';'\")\n",
    "            return \";\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4bb359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load with pandas, convert to Spark, and write to ABFS (overwrite)\n",
    "    def save_and_upload_to_abfs():\n",
    "        csv_bytes = download_dataset()\n",
    "        sample = csv_bytes.decode('utf-8')[:1000]\n",
    "        sep = detect_separator(sample)\n",
    "        print(f\"‚ÑπÔ∏è Detected separator: '{sep}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100d033f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load using pandas with detected or fallback separator\n",
    "        pdf = pd.read_csv(BytesIO(csv_bytes), sep=sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4908fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Spark DataFrame\n",
    "        df = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d79f484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to ABFS, always overwriting the folder\n",
    "        df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(ABFS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63bb113",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"‚úÖ File written to: {ABFS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1354e327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "    save_and_upload_to_abfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a3fa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-09T10:18:05.6518965Z\",\"execution_start_time\":\"2025-06-09T10:17:14.5512798Z\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\",\"parent_msg_id\":\"d452bd2e-8bd1-4307-a0a4-ddf6b5b8f452\",\"queued_time\":\"2025-06-09T10:16:40.7709374Z\",\"session_id\":\"7bc316bb-8190-4e95-b000-e8b4a46cece8\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"finished\",\"statement_id\":4,\"statement_ids\":[4]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3110f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "üîÑ Downloading full CSV from '3-consumos-faturados-por-municipio-ultimos-10-anos' ...\n",
    "    ‚ö†Ô∏è Could not auto-detect delimiter. Falling back to ';'\n",
    "    ‚ÑπÔ∏è Detected separator: ';'\n",
    "    ‚úÖ File written to: abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/3_consumos_faturados_por_municipio-ultimos_10_anos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0d886b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6eccee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] Monthly consumption by municipality' (Dataset ID: 3-consumos-faturados-por-municipio-ultimos-10-anos). Pausing before next extraction...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d25a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pause between dataset extraction commands ---\n",
    "    time_to_sleep = 60  # seconds (change if you want, e.g. 120 for extra caution)\n",
    "    for remaining in range(time_to_sleep, 0, -1):\n",
    "        print(f\"Sleeping for {remaining} seconds...\", end=\"\\r\")\n",
    "        time.sleep(1)\n",
    "    print(\"\\n[INFO] Sleep complete. Proceeding to next extraction.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e5871a",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-09T10:19:07.3013587Z\",\"execution_start_time\":\"2025-06-09T10:18:05.6539523Z\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\",\"parent_msg_id\":\"f1252f8a-7bde-4932-a4ba-3942d633d213\",\"queued_time\":\"2025-06-09T10:16:40.9191599Z\",\"session_id\":\"7bc316bb-8190-4e95-b000-e8b4a46cece8\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"finished\",\"statement_id\":5,\"statement_ids\":[5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2329bfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "[INFO] Monthly consumption by municipality' (Dataset ID: 3-consumos-faturados-por-municipio-ultimos-10-anos). Pausing before next extraction...\n",
    "    Sleeping for 1 seconds....\n",
    "    [INFO] Sleep complete. Proceeding to next extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1938fa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Monthly consumption by zip code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ec096b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset ID: 02_consumos_faturados_por_codigo_postal_ultimos_5_anos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa9064c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "    from datetime import datetime\n",
    "    from pyspark.sql import SparkSession\n",
    "    import pandas as pd\n",
    "    from io import BytesIO\n",
    "    import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2a4886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset name and ABFS path in the Lakehouse\n",
    "    DATASET_NAME = \"02-consumos-faturados-por-codigo-postal-ultimos-5-anos\"\n",
    "    ABFS_DIR = \"abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/02_consumos_faturados_por_codigo_postal_ultimos_5_anos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0a3c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Download the full CSV from the API\n",
    "    def download_dataset():\n",
    "        url = f\"https://e-redes.opendatasoft.com/api/explore/v2.1/catalog/datasets/{DATASET_NAME}/exports/csv\"\n",
    "        print(f\"üîÑ Downloading full CSV from '{DATASET_NAME}' ...\")\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"‚ùå Failed to export: {response.status_code}\")\n",
    "        return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152cc940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Automatically detect CSV delimiter with fallback\n",
    "    def detect_separator(sample_text):\n",
    "        sniffer = csv.Sniffer()\n",
    "        try:\n",
    "            dialect = sniffer.sniff(sample_text)\n",
    "            return dialect.delimiter\n",
    "        except csv.Error:\n",
    "            print(\"‚ö†Ô∏è Could not auto-detect delimiter. Falling back to ';'\")\n",
    "            return \";\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24079dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load with pandas, convert to Spark, and write to ABFS (overwrite)\n",
    "    def save_and_upload_to_abfs():\n",
    "        csv_bytes = download_dataset()\n",
    "        sample = csv_bytes.decode('utf-8')[:1000]\n",
    "        sep = detect_separator(sample)\n",
    "        print(f\"‚ÑπÔ∏è Detected separator: '{sep}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe87da8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load using pandas with detected or fallback separator\n",
    "        pdf = pd.read_csv(BytesIO(csv_bytes), sep=sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cccf787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Spark DataFrame\n",
    "        df = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a0f522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to ABFS, always overwriting the folder\n",
    "        df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(ABFS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9204d451",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"‚úÖ File written to: {ABFS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e018d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "    save_and_upload_to_abfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396d9682",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-09T10:19:30.2296121Z\",\"execution_start_time\":\"2025-06-09T10:19:07.3036656Z\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\",\"parent_msg_id\":\"e54ddb7f-888b-4c02-a3dd-36cce7daa85e\",\"queued_time\":\"2025-06-09T10:16:41.1028635Z\",\"session_id\":\"7bc316bb-8190-4e95-b000-e8b4a46cece8\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"finished\",\"statement_id\":6,\"statement_ids\":[6]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e031a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "üîÑ Downloading full CSV from '02-consumos-faturados-por-codigo-postal-ultimos-5-anos' ...\n",
    "    ‚ÑπÔ∏è Detected separator: ';'\n",
    "    ‚úÖ File written to: abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/02_consumos_faturados_por_codigo_postal_ultimos_5_anos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbc49f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7d4bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] Monthly consumption by zip code' (Dataset ID: 02-consumos-faturados-por-codigo-postal-ultimos-5-anos). Pausing before next extraction...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235d6c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pause between dataset extraction commands ---\n",
    "    time_to_sleep = 60  # seconds (change if you want, e.g. 120 for extra caution)\n",
    "    for remaining in range(time_to_sleep, 0, -1):\n",
    "        print(f\"Sleeping for {remaining} seconds...\", end=\"\\r\")\n",
    "        time.sleep(1)\n",
    "    print(\"\\n[INFO] Sleep complete. Proceeding to next extraction.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d53290b",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-09T10:20:30.5422189Z\",\"execution_start_time\":\"2025-06-09T10:19:30.231626Z\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\",\"parent_msg_id\":\"66738cf0-7cbb-49db-ae78-3eb086a04178\",\"queued_time\":\"2025-06-09T10:16:41.2897135Z\",\"session_id\":\"7bc316bb-8190-4e95-b000-e8b4a46cece8\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"finished\",\"statement_id\":7,\"statement_ids\":[7]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc2e05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "[INFO] Monthly consumption by zip code' (Dataset ID: 02-consumos-faturados-por-codigo-postal-ultimos-5-anos). Pausing before next extraction...\n",
    "    Sleeping for 1 seconds....\n",
    "    [INFO] Sleep complete. Proceeding to next extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0559343e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Substation Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f425c67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset ID: carga_na_subestacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef512ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "    from datetime import datetime\n",
    "    from pyspark.sql import SparkSession\n",
    "    import pandas as pd\n",
    "    from io import BytesIO\n",
    "    import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5619307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset name and ABFS path in the Lakehouse\n",
    "    DATASET_NAME = \"carga-na-subestacao\"\n",
    "    ABFS_DIR = \"abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/carga_na_subestacao\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b173d241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Download the full CSV from the API\n",
    "    def download_dataset():\n",
    "        url = f\"https://e-redes.opendatasoft.com/api/explore/v2.1/catalog/datasets/{DATASET_NAME}/exports/csv\"\n",
    "        print(f\"üîÑ Downloading full CSV from '{DATASET_NAME}' ...\")\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"‚ùå Failed to export: {response.status_code}\")\n",
    "        return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc542066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Automatically detect CSV delimiter with fallback\n",
    "    def detect_separator(sample_text):\n",
    "        sniffer = csv.Sniffer()\n",
    "        try:\n",
    "            dialect = sniffer.sniff(sample_text)\n",
    "            return dialect.delimiter\n",
    "        except csv.Error:\n",
    "            print(\"‚ö†Ô∏è Could not auto-detect delimiter. Falling back to ';'\")\n",
    "            return \";\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2a7c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load with pandas, convert to Spark, and write to ABFS (overwrite)\n",
    "    def save_and_upload_to_abfs():\n",
    "        csv_bytes = download_dataset()\n",
    "        sample = csv_bytes.decode('utf-8')[:1000]\n",
    "        sep = detect_separator(sample)\n",
    "        print(f\"‚ÑπÔ∏è Detected separator: '{sep}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79e6a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load using pandas with detected or fallback separator\n",
    "        pdf = pd.read_csv(BytesIO(csv_bytes), sep=sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eee7e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Spark DataFrame\n",
    "        df = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1923fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to ABFS, always overwriting the folder\n",
    "        df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(ABFS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd1ea89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"‚úÖ File written to: {ABFS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715a4c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "    save_and_upload_to_abfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09400aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-09T10:20:32.9918241Z\",\"execution_start_time\":\"2025-06-09T10:20:30.5440591Z\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\",\"parent_msg_id\":\"8e977b92-662d-466c-924e-a4650873cebe\",\"queued_time\":\"2025-06-09T10:16:41.5749773Z\",\"session_id\":\"7bc316bb-8190-4e95-b000-e8b4a46cece8\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"finished\",\"statement_id\":8,\"statement_ids\":[8]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5fff3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "üîÑ Downloading full CSV from 'carga-na-subestacao' ...\n",
    "    ‚ö†Ô∏è Could not auto-detect delimiter. Falling back to ';'\n",
    "    ‚ÑπÔ∏è Detected separator: ';'\n",
    "    ‚úÖ File written to: abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/carga_na_subestacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495d4278",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fbff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] Substation Load' (Dataset ID: carga-na-subestacao). Pausing before next extraction...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc766b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pause between dataset extraction commands ---\n",
    "    time_to_sleep = 60  # seconds (change if you want, e.g. 120 for extra caution)\n",
    "    for remaining in range(time_to_sleep, 0, -1):\n",
    "        print(f\"Sleeping for {remaining} seconds...\", end=\"\\r\")\n",
    "        time.sleep(1)\n",
    "    print(\"\\n[INFO] Sleep complete. Proceeding to next extraction.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f727d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-09T10:21:33.5808277Z\",\"execution_start_time\":\"2025-06-09T10:20:32.9937781Z\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\",\"parent_msg_id\":\"983723b2-983f-40f0-9323-f9d54e29fea6\",\"queued_time\":\"2025-06-09T10:16:41.8025038Z\",\"session_id\":\"7bc316bb-8190-4e95-b000-e8b4a46cece8\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"finished\",\"statement_id\":9,\"statement_ids\":[9]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39065c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "[INFO] Substation Load' (Dataset ID: carga-na-subestacao). Pausing before next extraction...\n",
    "    Sleeping for 1 seconds....\n",
    "    [INFO] Sleep complete. Proceeding to next extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265ef320",
   "metadata": {},
   "outputs": [],
   "source": [
    "Characterization of Public Lighting luminaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3d027f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset ID: cadastro_iluminacao_publica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0592aa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "    from datetime import datetime\n",
    "    from pyspark.sql import SparkSession\n",
    "    import pandas as pd\n",
    "    from io import BytesIO\n",
    "    import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fca8048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset name and ABFS path in the Lakehouse\n",
    "    DATASET_NAME = \"cadastro_iluminacao_publica\"\n",
    "    ABFS_DIR = \"abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/cadastro_iluminacao_publica\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668611bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Download the full CSV from the API\n",
    "    def download_dataset():\n",
    "        url = f\"https://e-redes.opendatasoft.com/api/explore/v2.1/catalog/datasets/{DATASET_NAME}/exports/csv\"\n",
    "        print(f\"üîÑ Downloading full CSV from '{DATASET_NAME}' ...\")\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"‚ùå Failed to export: {response.status_code}\")\n",
    "        return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6704984c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Automatically detect CSV delimiter with fallback\n",
    "    def detect_separator(sample_text):\n",
    "        sniffer = csv.Sniffer()\n",
    "        try:\n",
    "            dialect = sniffer.sniff(sample_text)\n",
    "            return dialect.delimiter\n",
    "        except csv.Error:\n",
    "            print(\"‚ö†Ô∏è Could not auto-detect delimiter. Falling back to ';'\")\n",
    "            return \";\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e19e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load with pandas, convert to Spark, and write to ABFS (overwrite)\n",
    "    def save_and_upload_to_abfs():\n",
    "        csv_bytes = download_dataset()\n",
    "        sample = csv_bytes.decode('utf-8')[:1000]\n",
    "        sep = detect_separator(sample)\n",
    "        print(f\"‚ÑπÔ∏è Detected separator: '{sep}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4080cfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load using pandas with detected or fallback separator\n",
    "        pdf = pd.read_csv(BytesIO(csv_bytes), sep=sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5de3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Spark DataFrame\n",
    "        df = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2529aead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to ABFS, always overwriting the folder\n",
    "        df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(ABFS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1ac158",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"‚úÖ File written to: {ABFS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0bf161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "    save_and_upload_to_abfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f13ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-09T10:21:37.1130078Z\",\"execution_start_time\":\"2025-06-09T10:21:33.5827185Z\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\",\"parent_msg_id\":\"73bec248-236e-4f3b-b91e-877c0d31f1b0\",\"queued_time\":\"2025-06-09T10:16:42.0036441Z\",\"session_id\":\"7bc316bb-8190-4e95-b000-e8b4a46cece8\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"finished\",\"statement_id\":10,\"statement_ids\":[10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ed8486",
   "metadata": {},
   "outputs": [],
   "source": [
    "üîÑ Downloading full CSV from 'cadastro_iluminacao_publica' ...\n",
    "    ‚ÑπÔ∏è Detected separator: ';'\n",
    "    ‚úÖ File written to: abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/cadastro_iluminacao_publica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ca3d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669637d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] Characterization of Public Lighting luminaires' (Dataset ID: cadastro_iluminacao_publica). Pausing before next extraction...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062e78ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pause between dataset extraction commands ---\n",
    "    time_to_sleep = 60  # seconds (change if you want, e.g. 120 for extra caution)\n",
    "    for remaining in range(time_to_sleep, 0, -1):\n",
    "        print(f\"Sleeping for {remaining} seconds...\", end=\"\\r\")\n",
    "        time.sleep(1)\n",
    "    print(\"\\n[INFO] Sleep complete. Proceeding to next extraction.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c0ce32",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-09T10:22:39.480384Z\",\"execution_start_time\":\"2025-06-09T10:21:37.1151758Z\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\",\"parent_msg_id\":\"349cedab-a10b-400c-aa2e-68c08870beee\",\"queued_time\":\"2025-06-09T10:16:42.1655583Z\",\"session_id\":\"7bc316bb-8190-4e95-b000-e8b4a46cece8\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"finished\",\"statement_id\":11,\"statement_ids\":[11]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bac35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "[INFO] Characterization of Public Lighting luminaires' (Dataset ID: cadastro_iluminacao_publica). Pausing before next extraction...\n",
    "    Sleeping for 1 seconds....\n",
    "    [INFO] Sleep complete. Proceeding to next extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437c1855",
   "metadata": {},
   "outputs": [],
   "source": [
    "Service continuity indicators by municipality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd159e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset ID:\n",
    "12_continuidade_de_servico_indicadores_gerais_de_continuidade_de_servico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e0f812",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "    from datetime import datetime\n",
    "    from pyspark.sql import SparkSession\n",
    "    import pandas as pd\n",
    "    from io import BytesIO\n",
    "    import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d35bf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset name and ABFS path in the Lakehouse\n",
    "    DATASET_NAME = \"12-continuidade-de-servico-indicadores-gerais-de-continuidade-de-servico\"\n",
    "    ABFS_DIR = \"abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/12_continuidade_de_servico_indicadores_gerais_de_continuidade_de_servico\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcf9ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Download the full CSV from the API\n",
    "    def download_dataset():\n",
    "        url = f\"https://e-redes.opendatasoft.com/api/explore/v2.1/catalog/datasets/{DATASET_NAME}/exports/csv\"\n",
    "        print(f\"üîÑ Downloading full CSV from '{DATASET_NAME}' ...\")\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"‚ùå Failed to export: {response.status_code}\")\n",
    "        return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f20d2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Automatically detect CSV delimiter with fallback\n",
    "    def detect_separator(sample_text):\n",
    "        sniffer = csv.Sniffer()\n",
    "        try:\n",
    "            dialect = sniffer.sniff(sample_text)\n",
    "            return dialect.delimiter\n",
    "        except csv.Error:\n",
    "            print(\"‚ö†Ô∏è Could not auto-detect delimiter. Falling back to ';'\")\n",
    "            return \";\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef24d488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load with pandas, convert to Spark, and write to ABFS (overwrite)\n",
    "    def save_and_upload_to_abfs():\n",
    "        csv_bytes = download_dataset()\n",
    "        sample = csv_bytes.decode('utf-8')[:1000]\n",
    "        sep = detect_separator(sample)\n",
    "        print(f\"‚ÑπÔ∏è Detected separator: '{sep}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be50806e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load using pandas with detected or fallback separator\n",
    "        pdf = pd.read_csv(BytesIO(csv_bytes), sep=sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e399c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Spark DataFrame\n",
    "        df = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cd3e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to ABFS, always overwriting the folder\n",
    "        df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(ABFS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffd9b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"‚úÖ File written to: {ABFS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6a6292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "    save_and_upload_to_abfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568dad9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-09T10:22:44.241336Z\",\"execution_start_time\":\"2025-06-09T10:22:39.4825887Z\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\",\"parent_msg_id\":\"ea6772ab-23e0-4a3d-b83e-df95793dd2d3\",\"queued_time\":\"2025-06-09T10:16:42.3766678Z\",\"session_id\":\"7bc316bb-8190-4e95-b000-e8b4a46cece8\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"finished\",\"statement_id\":12,\"statement_ids\":[12]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655fa014",
   "metadata": {},
   "outputs": [],
   "source": [
    "üîÑ Downloading full CSV from '12-continuidade-de-servico-indicadores-gerais-de-continuidade-de-servico' ...\n",
    "    ‚ÑπÔ∏è Detected separator: ';'\n",
    "    ‚úÖ File written to: abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/12_continuidade_de_servico_indicadores_gerais_de_continuidade_de_servico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b137cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffac228a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] Service continuity indicators by municipality' (Dataset ID: 12-continuidade-de-servico-indicadores-gerais-de-continuidade-de-servico). Pausing before next extraction...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02483faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pause between dataset extraction commands ---\n",
    "    time_to_sleep = 60  # seconds (change if you want, e.g. 120 for extra caution)\n",
    "    for remaining in range(time_to_sleep, 0, -1):\n",
    "        print(f\"Sleeping for {remaining} seconds...\", end=\"\\r\")\n",
    "        time.sleep(1)\n",
    "    print(\"\\n[INFO] Sleep complete. Proceeding to next extraction.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cbbf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-09T10:23:45.8296893Z\",\"execution_start_time\":\"2025-06-09T10:22:44.2434085Z\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\",\"parent_msg_id\":\"27ad3f4f-ae4b-42f9-86d7-9c305d0c8651\",\"queued_time\":\"2025-06-09T10:16:42.5656225Z\",\"session_id\":\"7bc316bb-8190-4e95-b000-e8b4a46cece8\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"finished\",\"statement_id\":13,\"statement_ids\":[13]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d642b271",
   "metadata": {},
   "outputs": [],
   "source": [
    "[INFO] Service continuity indicators by municipality' (Dataset ID: 12-continuidade-de-servico-indicadores-gerais-de-continuidade-de-servico). Pausing before next extraction...\n",
    "    Sleeping for 1 seconds....\n",
    "    [INFO] Sleep complete. Proceeding to next extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a7b6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "Outages Per Geography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec17e51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset ID: outages-per-geography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e865482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "    from datetime import datetime\n",
    "    from pyspark.sql import SparkSession\n",
    "    import pandas as pd\n",
    "    from io import BytesIO\n",
    "    import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd186c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset name and ABFS path in the Lakehouse\n",
    "    DATASET_NAME = \"outages-per-geography\"\n",
    "    ABFS_DIR = \"abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/outages_per_geography\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccfd501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Download the full CSV from the API\n",
    "    def download_dataset():\n",
    "        url = f\"https://e-redes.opendatasoft.com/api/explore/v2.1/catalog/datasets/{DATASET_NAME}/exports/csv\"\n",
    "        print(f\"üîÑ Downloading full CSV from '{DATASET_NAME}' ...\")\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"‚ùå Failed to export: {response.status_code}\")\n",
    "        return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ff1a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Automatically detect CSV delimiter with fallback\n",
    "    def detect_separator(sample_text):\n",
    "        sniffer = csv.Sniffer()\n",
    "        try:\n",
    "            dialect = sniffer.sniff(sample_text)\n",
    "            return dialect.delimiter\n",
    "        except csv.Error:\n",
    "            print(\"‚ö†Ô∏è Could not auto-detect delimiter. Falling back to ';'\")\n",
    "            return \";\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a9c146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load with pandas, convert to Spark, and write to ABFS (overwrite)\n",
    "    def save_and_upload_to_abfs():\n",
    "        csv_bytes = download_dataset()\n",
    "        sample = csv_bytes.decode('utf-8')[:1000]\n",
    "        sep = detect_separator(sample)\n",
    "        print(f\"‚ÑπÔ∏è Detected separator: '{sep}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a72cf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load using pandas with detected or fallback separator\n",
    "        pdf = pd.read_csv(BytesIO(csv_bytes), sep=sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bd096d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Spark DataFrame\n",
    "        df = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d921a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to ABFS, always overwriting the folder\n",
    "        df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(ABFS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20d9e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"‚úÖ File written to: {ABFS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0260ed65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "    save_and_upload_to_abfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671d762d",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-09T10:23:48.1771737Z\",\"execution_start_time\":\"2025-06-09T10:23:45.8385985Z\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\",\"parent_msg_id\":\"504f82cb-4db4-4c6c-84cf-ad2cbccacba8\",\"queued_time\":\"2025-06-09T10:16:42.7793382Z\",\"session_id\":\"7bc316bb-8190-4e95-b000-e8b4a46cece8\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"finished\",\"statement_id\":14,\"statement_ids\":[14]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b973b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "üîÑ Downloading full CSV from 'outages-per-geography' ...\n",
    "    ‚ÑπÔ∏è Detected separator: ';'\n",
    "    ‚úÖ File written to: abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/outages_per_geography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69d60ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3fc610",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] Active Energy Interruptions' (Dataset ID: outages-per-geography). Pausing before next extraction...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05adf71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pause between dataset extraction commands ---\n",
    "    time_to_sleep = 60  # seconds (change if you want, e.g. 120 for extra caution)\n",
    "    for remaining in range(time_to_sleep, 0, -1):\n",
    "        print(f\"Sleeping for {remaining} seconds...\", end=\"\\r\")\n",
    "        time.sleep(1)\n",
    "    print(\"\\n[INFO] Sleep complete. Proceeding to next extraction.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3962d56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-09T10:24:48.5842372Z\",\"execution_start_time\":\"2025-06-09T10:23:48.1791656Z\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\",\"parent_msg_id\":\"5b368681-356a-4cf8-af3a-da2b153c676e\",\"queued_time\":\"2025-06-09T10:16:42.9319247Z\",\"session_id\":\"7bc316bb-8190-4e95-b000-e8b4a46cece8\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"finished\",\"statement_id\":15,\"statement_ids\":[15]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7d620d",
   "metadata": {},
   "outputs": [],
   "source": [
    "[INFO] Active Energy Interruptions' (Dataset ID: outages-per-geography). Pausing before next extraction...\n",
    "    Sleeping for 1 seconds....\n",
    "    [INFO] Sleep complete. Proceeding to next extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866ec049",
   "metadata": {},
   "outputs": [],
   "source": [
    "Total production Units for Self-Consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1ca012",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset ID: 8_unidades_de_producao_para_autoconsumo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26952ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "    from datetime import datetime\n",
    "    from pyspark.sql import SparkSession\n",
    "    import pandas as pd\n",
    "    from io import BytesIO\n",
    "    import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ce3dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset name and ABFS path in the Lakehouse\n",
    "    DATASET_NAME = \"8-unidades-de-producao-para-autoconsumo\"\n",
    "    ABFS_DIR = \"abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/8_unidades_de_producao_para_autoconsumo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e250cc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Download the full CSV from the API\n",
    "    def download_dataset():\n",
    "        url = f\"https://e-redes.opendatasoft.com/api/explore/v2.1/catalog/datasets/{DATASET_NAME}/exports/csv\"\n",
    "        print(f\"üîÑ Downloading full CSV from '{DATASET_NAME}' ...\")\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"‚ùå Failed to export: {response.status_code}\")\n",
    "        return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0121e2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Automatically detect CSV delimiter with fallback\n",
    "    def detect_separator(sample_text):\n",
    "        sniffer = csv.Sniffer()\n",
    "        try:\n",
    "            dialect = sniffer.sniff(sample_text)\n",
    "            return dialect.delimiter\n",
    "        except csv.Error:\n",
    "            print(\"‚ö†Ô∏è Could not auto-detect delimiter. Falling back to ';'\")\n",
    "            return \";\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9650512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load with pandas, convert to Spark, and write to ABFS (overwrite)\n",
    "    def save_and_upload_to_abfs():\n",
    "        csv_bytes = download_dataset()\n",
    "        sample = csv_bytes.decode('utf-8')[:1000]\n",
    "        sep = detect_separator(sample)\n",
    "        print(f\"‚ÑπÔ∏è Detected separator: '{sep}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dfd508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load using pandas with detected or fallback separator\n",
    "        pdf = pd.read_csv(BytesIO(csv_bytes), sep=sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020dbc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Spark DataFrame\n",
    "        df = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f49e328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to ABFS, always overwriting the folder\n",
    "        df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(ABFS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e4962f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"‚úÖ File written to: {ABFS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e2b789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "    save_and_upload_to_abfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b524c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-09T10:25:07.6887561Z\",\"execution_start_time\":\"2025-06-09T10:24:48.5866473Z\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\",\"parent_msg_id\":\"33042b0a-d565-431f-82b9-6d0aaba2d9d3\",\"queued_time\":\"2025-06-09T10:16:43.1947905Z\",\"session_id\":\"7bc316bb-8190-4e95-b000-e8b4a46cece8\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"finished\",\"statement_id\":16,\"statement_ids\":[16]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f914df7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "üîÑ Downloading full CSV from '8-unidades-de-producao-para-autoconsumo' ...\n",
    "    ‚ÑπÔ∏è Detected separator: ';'\n",
    "    ‚úÖ File written to: abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/8_unidades_de_producao_para_autoconsumo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08012f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f421a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] Total production Units for Self-Consumption' (Dataset ID: 8-unidades-de-producao-para-autoconsumo). Pausing before next extraction...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a12820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pause between dataset extraction commands ---\n",
    "    time_to_sleep = 60  # seconds (change if you want, e.g. 120 for extra caution)\n",
    "    for remaining in range(time_to_sleep, 0, -1):\n",
    "        print(f\"Sleeping for {remaining} seconds...\", end=\"\\r\")\n",
    "        time.sleep(1)\n",
    "    print(\"\\n[INFO] Sleep complete. Proceeding to next extraction.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a93c64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-09T10:26:08.6458235Z\",\"execution_start_time\":\"2025-06-09T10:25:07.6908377Z\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\",\"parent_msg_id\":\"cd4763e4-2aa8-409e-a7e4-037de2be22eb\",\"queued_time\":\"2025-06-09T10:16:43.371818Z\",\"session_id\":\"7bc316bb-8190-4e95-b000-e8b4a46cece8\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"finished\",\"statement_id\":17,\"statement_ids\":[17]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf01d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "[INFO] Total production Units for Self-Consumption' (Dataset ID: 8-unidades-de-producao-para-autoconsumo). Pausing before next extraction...\n",
    "    Sleeping for 1 seconds....\n",
    "    [INFO] Sleep complete. Proceeding to next extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4e0e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "Network Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fa4d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset ID: caracteristicas_da_rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5462760e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "    from datetime import datetime\n",
    "    from pyspark.sql import SparkSession\n",
    "    import pandas as pd\n",
    "    from io import BytesIO\n",
    "    import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e838e254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset name and ABFS path in the Lakehouse\n",
    "    DATASET_NAME = \"caracteristicas-da-rede\"\n",
    "    ABFS_DIR = \"abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/caracteristicas_da_rede\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd27f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Download the full CSV from the API\n",
    "    def download_dataset():\n",
    "        url = f\"https://e-redes.opendatasoft.com/api/explore/v2.1/catalog/datasets/{DATASET_NAME}/exports/csv\"\n",
    "        print(f\"üîÑ Downloading full CSV from '{DATASET_NAME}' ...\")\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"‚ùå Failed to export: {response.status_code}\")\n",
    "        return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d04e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Automatically detect CSV delimiter with fallback\n",
    "    def detect_separator(sample_text):\n",
    "        sniffer = csv.Sniffer()\n",
    "        try:\n",
    "            dialect = sniffer.sniff(sample_text)\n",
    "            return dialect.delimiter\n",
    "        except csv.Error:\n",
    "            print(\"‚ö†Ô∏è Could not auto-detect delimiter. Falling back to ';'\")\n",
    "            return \";\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1abee6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load with pandas, convert to Spark, and write to ABFS (overwrite)\n",
    "    def save_and_upload_to_abfs():\n",
    "        csv_bytes = download_dataset()\n",
    "        sample = csv_bytes.decode('utf-8')[:1000]\n",
    "        sep = detect_separator(sample)\n",
    "        print(f\"‚ÑπÔ∏è Detected separator: '{sep}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247dffae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load using pandas with detected or fallback separator\n",
    "        pdf = pd.read_csv(BytesIO(csv_bytes), sep=sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af35e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Spark DataFrame\n",
    "        df = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6868889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to ABFS, always overwriting the folder\n",
    "        df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(ABFS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f103bd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"‚úÖ File written to: {ABFS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2fc54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "    save_and_upload_to_abfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb75737b",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-09T10:26:11.02954Z\",\"execution_start_time\":\"2025-06-09T10:26:08.6480884Z\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\",\"parent_msg_id\":\"2848c6af-a72e-467c-bf57-78a2e31f4646\",\"queued_time\":\"2025-06-09T10:16:43.5986582Z\",\"session_id\":\"7bc316bb-8190-4e95-b000-e8b4a46cece8\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"finished\",\"statement_id\":18,\"statement_ids\":[18]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d401a319",
   "metadata": {},
   "outputs": [],
   "source": [
    "üîÑ Downloading full CSV from 'caracteristicas-da-rede' ...\n",
    "    ‚ö†Ô∏è Could not auto-detect delimiter. Falling back to ';'\n",
    "    ‚ÑπÔ∏è Detected separator: ';'\n",
    "    ‚úÖ File written to: abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/caracteristicas_da_rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800cb28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5520d541",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] Network Features' (Dataset ID: caracteristicas-da-rede). Pausing before next extraction...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb82152a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pause between dataset extraction commands ---\n",
    "    time_to_sleep = 60  # seconds (change if you want, e.g. 120 for extra caution)\n",
    "    for remaining in range(time_to_sleep, 0, -1):\n",
    "        print(f\"Sleeping for {remaining} seconds...\", end=\"\\r\")\n",
    "        time.sleep(1)\n",
    "    print(\"\\n[INFO] Sleep complete. Proceeding to next extraction.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edfd479",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-09T10:27:12.2454731Z\",\"execution_start_time\":\"2025-06-09T10:26:11.0314989Z\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\",\"parent_msg_id\":\"e1f3ce45-fadb-4176-96a2-2e736ae08534\",\"queued_time\":\"2025-06-09T10:16:43.8569112Z\",\"session_id\":\"7bc316bb-8190-4e95-b000-e8b4a46cece8\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"finished\",\"statement_id\":19,\"statement_ids\":[19]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3f1870",
   "metadata": {},
   "outputs": [],
   "source": [
    "[INFO] Network Features' (Dataset ID: caracteristicas-da-rede). Pausing before next extraction...\n",
    "    Sleeping for 1 seconds....\n",
    "    [INFO] Sleep complete. Proceeding to next extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23068628",
   "metadata": {},
   "outputs": [],
   "source": [
    "Consumption Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4316587f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset ID: previsao_de_consumo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f071d1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "    from datetime import datetime\n",
    "    from pyspark.sql import SparkSession\n",
    "    import pandas as pd\n",
    "    from io import BytesIO\n",
    "    import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fe109b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset name and ABFS path in the Lakehouse\n",
    "    DATASET_NAME = \"previsao-de-consumo\"\n",
    "    ABFS_DIR = \"abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/previsao_de_consumo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddcfc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Download the full CSV from the API\n",
    "    def download_dataset():\n",
    "        url = f\"https://e-redes.opendatasoft.com/api/explore/v2.1/catalog/datasets/{DATASET_NAME}/exports/csv\"\n",
    "        print(f\"üîÑ Downloading full CSV from '{DATASET_NAME}' ...\")\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"‚ùå Failed to export: {response.status_code}\")\n",
    "        return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b428bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Automatically detect CSV delimiter with fallback\n",
    "    def detect_separator(sample_text):\n",
    "        sniffer = csv.Sniffer()\n",
    "        try:\n",
    "            dialect = sniffer.sniff(sample_text)\n",
    "            return dialect.delimiter\n",
    "        except csv.Error:\n",
    "            print(\"‚ö†Ô∏è Could not auto-detect delimiter. Falling back to ';'\")\n",
    "            return \";\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a73479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load with pandas, convert to Spark, and write to ABFS (overwrite)\n",
    "    def save_and_upload_to_abfs():\n",
    "        csv_bytes = download_dataset()\n",
    "        sample = csv_bytes.decode('utf-8')[:1000]\n",
    "        sep = detect_separator(sample)\n",
    "        print(f\"‚ÑπÔ∏è Detected separator: '{sep}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1624e109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load using pandas with detected or fallback separator\n",
    "        pdf = pd.read_csv(BytesIO(csv_bytes), sep=sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87119772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Spark DataFrame\n",
    "        df = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d49a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to ABFS, always overwriting the folder\n",
    "        df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(ABFS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6056bfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"‚úÖ File written to: {ABFS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28af4708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "    save_and_upload_to_abfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63197c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-09T10:27:23.4871666Z\",\"execution_start_time\":\"2025-06-09T10:27:12.2475641Z\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\",\"parent_msg_id\":\"360e2b8c-2b23-4554-a3ef-ba0dae9201b8\",\"queued_time\":\"2025-06-09T10:16:44.2036847Z\",\"session_id\":\"7bc316bb-8190-4e95-b000-e8b4a46cece8\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"finished\",\"statement_id\":20,\"statement_ids\":[20]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbb7b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "üîÑ Downloading full CSV from 'previsao-de-consumo' ...\n",
    "    ‚ÑπÔ∏è Detected separator: ';'\n",
    "    ‚úÖ File written to: abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/previsao_de_consumo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06eab698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd83a091",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] Consumption Forecast (Dataset ID: previsao-de-consumo). Pausing before next extraction...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b703809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pause between dataset extraction commands ---\n",
    "    time_to_sleep = 60  # seconds (change if you want, e.g. 120 for extra caution)\n",
    "    for remaining in range(time_to_sleep, 0, -1):\n",
    "        print(f\"Sleeping for {remaining} seconds...\", end=\"\\r\")\n",
    "        time.sleep(1)\n",
    "    print(\"\\n[INFO] Sleep complete. Proceeding to next extraction.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec515d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-09T10:28:25.0496116Z\",\"execution_start_time\":\"2025-06-09T10:27:23.4890464Z\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\",\"parent_msg_id\":\"abd636dd-02ef-4dbd-9936-7d81b1e46d67\",\"queued_time\":\"2025-06-09T10:16:44.7355771Z\",\"session_id\":\"7bc316bb-8190-4e95-b000-e8b4a46cece8\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"finished\",\"statement_id\":21,\"statement_ids\":[21]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e930b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "[INFO] Consumption Forecast (Dataset ID: previsao-de-consumo). Pausing before next extraction...\n",
    "    Sleeping for 1 seconds....\n",
    "    [INFO] Sleep complete. Proceeding to next extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54219c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Number of low voltage consumption locations with Remote Reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b698ffe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset ID: 23_leituras_recolhidas_remotamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e87163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "    from datetime import datetime\n",
    "    from pyspark.sql import SparkSession\n",
    "    import pandas as pd\n",
    "    from io import BytesIO\n",
    "    import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e766de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset name and ABFS path in the Lakehouse\n",
    "    DATASET_NAME = \"23-leituras-recolhidas-remotamente\"\n",
    "    ABFS_DIR = \"abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/23_leituras_recolhidas_remotamente\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f4f0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Download the full CSV from the API\n",
    "    def download_dataset():\n",
    "        url = f\"https://e-redes.opendatasoft.com/api/explore/v2.1/catalog/datasets/{DATASET_NAME}/exports/csv\"\n",
    "        print(f\"üîÑ Downloading full CSV from '{DATASET_NAME}' ...\")\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"‚ùå Failed to export: {response.status_code}\")\n",
    "        return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2636474a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Automatically detect CSV delimiter with fallback\n",
    "    def detect_separator(sample_text):\n",
    "        sniffer = csv.Sniffer()\n",
    "        try:\n",
    "            dialect = sniffer.sniff(sample_text)\n",
    "            return dialect.delimiter\n",
    "        except csv.Error:\n",
    "            print(\"‚ö†Ô∏è Could not auto-detect delimiter. Falling back to ';'\")\n",
    "            return \";\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015f9dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load with pandas, convert to Spark, and write to ABFS (overwrite)\n",
    "    def save_and_upload_to_abfs():\n",
    "        csv_bytes = download_dataset()\n",
    "        sample = csv_bytes.decode('utf-8')[:1000]\n",
    "        sep = detect_separator(sample)\n",
    "        print(f\"‚ÑπÔ∏è Detected separator: '{sep}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7393d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load using pandas with detected or fallback separator\n",
    "        pdf = pd.read_csv(BytesIO(csv_bytes), sep=sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0588a560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Spark DataFrame\n",
    "        df = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c15e561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to ABFS, always overwriting the folder\n",
    "        df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(ABFS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5e7646",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"‚úÖ File written to: {ABFS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00997a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "    save_and_upload_to_abfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657b0737",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-09T10:28:39.733447Z\",\"execution_start_time\":\"2025-06-09T10:28:25.0518Z\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\",\"parent_msg_id\":\"081f7ddf-0527-4de5-af2c-0122121fe80f\",\"queued_time\":\"2025-06-09T10:16:45.082606Z\",\"session_id\":\"7bc316bb-8190-4e95-b000-e8b4a46cece8\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"finished\",\"statement_id\":22,\"statement_ids\":[22]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae54b065",
   "metadata": {},
   "outputs": [],
   "source": [
    "üîÑ Downloading full CSV from '23-leituras-recolhidas-remotamente' ...\n",
    "    ‚ÑπÔ∏è Detected separator: ';'\n",
    "    ‚úÖ File written to: abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/23_leituras_recolhidas_remotamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101c2df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5197e893",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] Consumption Forecast (Dataset ID: previsao-de-consumo). Pausing before next extraction...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b25399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pause between dataset extraction commands ---\n",
    "    time_to_sleep = 60  # seconds (change if you want, e.g. 120 for extra caution)\n",
    "    for remaining in range(time_to_sleep, 0, -1):\n",
    "        print(f\"Sleeping for {remaining} seconds...\", end=\"\\r\")\n",
    "        time.sleep(1)\n",
    "    print(\"\\n[INFO] Sleep complete. Proceeding to next extraction.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8781b03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-09T10:29:41.6886395Z\",\"execution_start_time\":\"2025-06-09T10:28:39.7355212Z\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\",\"parent_msg_id\":\"c45bbe21-4429-43d3-a741-6f08ca3e3f4d\",\"queued_time\":\"2025-06-09T10:16:45.3627589Z\",\"session_id\":\"7bc316bb-8190-4e95-b000-e8b4a46cece8\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"finished\",\"statement_id\":23,\"statement_ids\":[23]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a207b67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "[INFO] Consumption Forecast (Dataset ID: previsao-de-consumo). Pausing before next extraction...\n",
    "    Sleeping for 1 seconds....\n",
    "    [INFO] Sleep complete. Proceeding to next extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e7a559",
   "metadata": {},
   "outputs": [],
   "source": [
    "Number of remote work orders carried out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910620e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset ID: 15_ordens_de_servico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc76271",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "    from datetime import datetime\n",
    "    from pyspark.sql import SparkSession\n",
    "    import pandas as pd\n",
    "    from io import BytesIO\n",
    "    import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180084b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset name and ABFS path in the Lakehouse\n",
    "    DATASET_NAME = \"15-ordens-de-servico\"\n",
    "    ABFS_DIR = \"abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/15_ordens_de_servico\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450f2aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Download the full CSV from the API\n",
    "    def download_dataset():\n",
    "        url = f\"https://e-redes.opendatasoft.com/api/explore/v2.1/catalog/datasets/{DATASET_NAME}/exports/csv\"\n",
    "        print(f\"üîÑ Downloading full CSV from '{DATASET_NAME}' ...\")\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"‚ùå Failed to export: {response.status_code}\")\n",
    "        return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45483186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Automatically detect CSV delimiter with fallback\n",
    "    def detect_separator(sample_text):\n",
    "        sniffer = csv.Sniffer()\n",
    "        try:\n",
    "            dialect = sniffer.sniff(sample_text)\n",
    "            return dialect.delimiter\n",
    "        except csv.Error:\n",
    "            print(\"‚ö†Ô∏è Could not auto-detect delimiter. Falling back to ';'\")\n",
    "            return \";\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6f4fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load with pandas, convert to Spark, and write to ABFS (overwrite)\n",
    "    def save_and_upload_to_abfs():\n",
    "        csv_bytes = download_dataset()\n",
    "        sample = csv_bytes.decode('utf-8')[:1000]\n",
    "        sep = detect_separator(sample)\n",
    "        print(f\"‚ÑπÔ∏è Detected separator: '{sep}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d81727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load using pandas with detected or fallback separator\n",
    "        pdf = pd.read_csv(BytesIO(csv_bytes), sep=sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98869bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Spark DataFrame\n",
    "        df = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7582e395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to ABFS, always overwriting the folder\n",
    "        df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(ABFS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115f57be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"‚úÖ File written to: {ABFS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640126fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "    save_and_upload_to_abfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0330f0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-09T10:31:05.1592802Z\",\"execution_start_time\":\"2025-06-09T10:29:41.6908127Z\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\",\"parent_msg_id\":\"15dd3985-060c-4675-9a69-e93a747a4a87\",\"queued_time\":\"2025-06-09T10:16:45.6246293Z\",\"session_id\":\"7bc316bb-8190-4e95-b000-e8b4a46cece8\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"finished\",\"statement_id\":24,\"statement_ids\":[24]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff069bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "üîÑ Downloading full CSV from '15-ordens-de-servico' ...\n",
    "    ‚ÑπÔ∏è Detected separator: ';'\n",
    "    ‚úÖ File written to: abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/15-ordens-de-servico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a124c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6131486c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] Number of remote work orders carried out (Dataset ID: 15-ordens-de-servico). Pausing before next extraction...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05beb0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pause between dataset extraction commands ---\n",
    "    time_to_sleep = 60  # seconds (change if you want, e.g. 120 for extra caution)\n",
    "    for remaining in range(time_to_sleep, 0, -1):\n",
    "        print(f\"Sleeping for {remaining} seconds...\", end=\"\\r\")\n",
    "        time.sleep(1)\n",
    "    print(\"\\n[INFO] Sleep complete. Proceeding to next extraction.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b19346",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022478cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-09T10:32:06.6913085Z\",\"execution_start_time\":\"2025-06-09T10:31:05.1612453Z\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\",\"parent_msg_id\":\"49ac6310-a00d-4ff6-b05a-1f974bc52554\",\"queued_time\":\"2025-06-09T10:16:45.8648265Z\",\"session_id\":\"7bc316bb-8190-4e95-b000-e8b4a46cece8\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"finished\",\"statement_id\":25,\"statement_ids\":[25]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fa09e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "[INFO] Number of remote work orders carried out (Dataset ID: 15-ordens-de-servico). Pausing before next extraction...\n",
    "    Sleeping for 1 seconds....\n",
    "    [INFO] Sleep complete. Proceeding to next extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdde6b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "Scheduled Energy interruptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938ae652",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset ID: network_scheduling_work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf0ee9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "    from datetime import datetime\n",
    "    from pyspark.sql import SparkSession\n",
    "    import pandas as pd\n",
    "    from io import BytesIO\n",
    "    import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e00711a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset name and ABFS path in the Lakehouse\n",
    "    DATASET_NAME = \"network-scheduling-work\"\n",
    "    ABFS_DIR = \"abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/network_scheduling_work\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69681fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Download the full CSV from the API\n",
    "    def download_dataset():\n",
    "        url = f\"https://e-redes.opendatasoft.com/api/explore/v2.1/catalog/datasets/{DATASET_NAME}/exports/csv\"\n",
    "        print(f\"üîÑ Downloading full CSV from '{DATASET_NAME}' ...\")\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"‚ùå Failed to export: {response.status_code}\")\n",
    "        return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c921900f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Detect separator from sample, with fallback to ';'\n",
    "    def detect_separator(sample_text):\n",
    "        sniffer = csv.Sniffer()\n",
    "        try:\n",
    "            dialect = sniffer.sniff(sample_text)\n",
    "            detected = dialect.delimiter\n",
    "            if detected not in [';', ',', '\\t', '|']:\n",
    "                print(f\"‚ö†Ô∏è Detected invalid delimiter '{detected}', falling back to ';'\")\n",
    "                return ';'\n",
    "            return detected\n",
    "        except csv.Error:\n",
    "            print(\"‚ö†Ô∏è Could not auto-detect delimiter. Falling back to ';'\")\n",
    "            return ';'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14891483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load with pandas, convert to Spark, and write to ABFS (overwrite)\n",
    "    def save_and_upload_to_abfs():\n",
    "        csv_bytes = download_dataset()\n",
    "        sample = csv_bytes.decode('utf-8', errors='ignore')[:1000]\n",
    "        sep = detect_separator(sample)\n",
    "        print(f\"‚ÑπÔ∏è Using separator: '{sep}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412e9e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load using pandas\n",
    "        pdf = pd.read_csv(BytesIO(csv_bytes), sep=sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275d8477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session\n",
    "        spark = SparkSession.builder.getOrCreate()\n",
    "        df = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f48293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to ABFS\n",
    "        df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(ABFS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83da5071",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"‚úÖ File written to: {ABFS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38972a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "    save_and_upload_to_abfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc15052a",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-09T12:59:18.8786714Z\",\"execution_start_time\":\"2025-06-09T12:59:13.5460741Z\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\",\"parent_msg_id\":\"0ec7a4f6-3f21-4729-aeaf-13ed8de8c127\",\"queued_time\":\"2025-06-09T12:59:02.1571776Z\",\"session_id\":\"72fc3f29-5d1e-4a24-8838-9446385e24bb\",\"session_start_time\":\"2025-06-09T12:59:02.1582998Z\",\"spark_pool\":null,\"state\":\"finished\",\"statement_id\":3,\"statement_ids\":[3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c0d343",
   "metadata": {},
   "outputs": [],
   "source": [
    "üîÑ Downloading full CSV from 'network-scheduling-work' ...\n",
    "    ‚ö†Ô∏è Detected invalid delimiter 'g', falling back to ';'\n",
    "    ‚ÑπÔ∏è Using separator: ';'\n",
    "    ‚úÖ File written to: abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/network_scheduling_work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018bfb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06fc0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] Scheduled Energy interruptions (Dataset ID: network-scheduling-work). Pausing before next extraction...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc96867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pause between dataset extraction commands ---\n",
    "    time_to_sleep = 60  # seconds (change if you want, e.g. 120 for extra caution)\n",
    "    for remaining in range(time_to_sleep, 0, -1):\n",
    "        print(f\"Sleeping for {remaining} seconds...\", end=\"\\r\")\n",
    "        time.sleep(1)\n",
    "    print(\"\\n[INFO] Sleep complete. Proceeding to next extraction.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08e71c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec59c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-09T13:00:19.6247635Z\",\"execution_start_time\":\"2025-06-09T12:59:18.8808652Z\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\",\"parent_msg_id\":\"365794ed-3c3f-409d-a48e-65304f05c5b9\",\"queued_time\":\"2025-06-09T12:59:02.1588831Z\",\"session_id\":\"72fc3f29-5d1e-4a24-8838-9446385e24bb\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"finished\",\"statement_id\":4,\"statement_ids\":[4]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d764ee27",
   "metadata": {},
   "outputs": [],
   "source": [
    "[INFO] Scheduled Energy interruptions (Dataset ID: network-scheduling-work). Pausing before next extraction...\n",
    "    Sleeping for 1 seconds....\n",
    "    [INFO] Sleep complete. Proceeding to next extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33b2fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "Number of active energy contracts by meter type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd11a5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset ID: 21_contadores_de_energia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e069d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "    from datetime import datetime\n",
    "    from pyspark.sql import SparkSession\n",
    "    import pandas as pd\n",
    "    from io import BytesIO\n",
    "    import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e8983c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset name and ABFS path in the Lakehouse\n",
    "    DATASET_NAME = \"21-contadores-de-energia\"\n",
    "    ABFS_DIR = \"abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/21_contadores_de_energia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff02244c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Download the full CSV from the API\n",
    "    def download_dataset():\n",
    "        url = f\"https://e-redes.opendatasoft.com/api/explore/v2.1/catalog/datasets/{DATASET_NAME}/exports/csv\"\n",
    "        print(f\"üîÑ Downloading full CSV from '{DATASET_NAME}' ...\")\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"‚ùå Failed to export: {response.status_code}\")\n",
    "        return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b0705f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Automatically detect CSV delimiter with fallback\n",
    "    def detect_separator(sample_text):\n",
    "        sniffer = csv.Sniffer()\n",
    "        try:\n",
    "            dialect = sniffer.sniff(sample_text)\n",
    "            return dialect.delimiter\n",
    "        except csv.Error:\n",
    "            print(\"‚ö†Ô∏è Could not auto-detect delimiter. Falling back to ';'\")\n",
    "            return \";\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1aad6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load with pandas, convert to Spark, and write to ABFS (overwrite)\n",
    "    def save_and_upload_to_abfs():\n",
    "        csv_bytes = download_dataset()\n",
    "        sample = csv_bytes.decode('utf-8')[:1000]\n",
    "        sep = detect_separator(sample)\n",
    "        print(f\"‚ÑπÔ∏è Detected separator: '{sep}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ded2eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load using pandas with detected or fallback separator\n",
    "        pdf = pd.read_csv(BytesIO(csv_bytes), sep=sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3630a0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Spark DataFrame\n",
    "        df = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd2e65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to ABFS, always overwriting the folder\n",
    "        df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(ABFS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b371103",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"‚úÖ File written to: {ABFS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cce010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "    save_and_upload_to_abfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfad48e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-09T13:01:18.4396456Z\",\"execution_start_time\":\"2025-06-09T13:00:19.6270754Z\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\",\"parent_msg_id\":\"f4d6a44b-7f97-495e-8a10-83b09cb257b7\",\"queued_time\":\"2025-06-09T12:59:02.1605348Z\",\"session_id\":\"72fc3f29-5d1e-4a24-8838-9446385e24bb\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"finished\",\"statement_id\":5,\"statement_ids\":[5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75798d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "üîÑ Downloading full CSV from '21-contadores-de-energia' ...\n",
    "    ‚ÑπÔ∏è Detected separator: ';'\n",
    "    ‚úÖ File written to: abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/21_contadores_de_energia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9100456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dbd6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] Number of active energy contracts by meter type (Dataset ID: 21-contadores-de-energia). Pausing before next extraction...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfa04b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pause between dataset extraction commands ---\n",
    "    time_to_sleep = 60  # seconds (change if you want, e.g. 120 for extra caution)\n",
    "    for remaining in range(time_to_sleep, 0, -1):\n",
    "        print(f\"Sleeping for {remaining} seconds...\", end=\"\\r\")\n",
    "        time.sleep(1)\n",
    "    print(\"\\n[INFO] Sleep complete. Proceeding to next extraction.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066db287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba9b1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-09T13:02:18.688522Z\",\"execution_start_time\":\"2025-06-09T13:01:18.4419151Z\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\",\"parent_msg_id\":\"cd6320ba-b67d-446e-a63e-51646f67c891\",\"queued_time\":\"2025-06-09T12:59:02.1621497Z\",\"session_id\":\"72fc3f29-5d1e-4a24-8838-9446385e24bb\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"finished\",\"statement_id\":6,\"statement_ids\":[6]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99add4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "[INFO] Number of active energy contracts by meter type (Dataset ID: 21-contadores-de-energia). Pausing before next extraction...\n",
    "    Sleeping for 1 seconds....\n",
    "    [INFO] Sleep complete. Proceeding to next extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcf86f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Characterization of Consumption Points (CPEs), with active contracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c273d5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset ID: 20_caracterizacao_pes_contrato_ativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498343a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "    from datetime import datetime\n",
    "    from pyspark.sql import SparkSession\n",
    "    import pandas as pd\n",
    "    from io import BytesIO\n",
    "    import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b9c956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset name and ABFS path in the Lakehouse\n",
    "    DATASET_NAME = \"20-caracterizacao-pes-contrato-ativo\"\n",
    "    ABFS_DIR = \"abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/20_caracterizacao_pes_contrato_ativo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f10f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Download the full CSV from the API\n",
    "    def download_dataset():\n",
    "        url = f\"https://e-redes.opendatasoft.com/api/explore/v2.1/catalog/datasets/{DATASET_NAME}/exports/csv\"\n",
    "        print(f\"üîÑ Downloading full CSV from '{DATASET_NAME}' ...\")\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"‚ùå Failed to export: {response.status_code}\")\n",
    "        return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600fa733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Automatically detect CSV delimiter with fallback\n",
    "    def detect_separator(sample_text):\n",
    "        sniffer = csv.Sniffer()\n",
    "        try:\n",
    "            dialect = sniffer.sniff(sample_text)\n",
    "            return dialect.delimiter\n",
    "        except csv.Error:\n",
    "            print(\"‚ö†Ô∏è Could not auto-detect delimiter. Falling back to ';'\")\n",
    "            return \";\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb32e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load with pandas, convert to Spark, and write to ABFS (overwrite)\n",
    "    def save_and_upload_to_abfs():\n",
    "        csv_bytes = download_dataset()\n",
    "        sample = csv_bytes.decode('utf-8')[:1000]\n",
    "        sep = detect_separator(sample)\n",
    "        print(f\"‚ÑπÔ∏è Detected separator: '{sep}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e39d049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load using pandas with detected or fallback separator\n",
    "        pdf = pd.read_csv(BytesIO(csv_bytes), sep=sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312c6a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Spark DataFrame\n",
    "        df = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1d899e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to ABFS, always overwriting the folder\n",
    "        df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(ABFS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81447e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"‚úÖ File written to: {ABFS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6171d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "    save_and_upload_to_abfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0300fe4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-09T13:04:14.5168934Z\",\"execution_start_time\":\"2025-06-09T13:02:18.6907481Z\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\",\"parent_msg_id\":\"d7a110fb-3eb8-4ff5-a2e1-75ed92671865\",\"queued_time\":\"2025-06-09T12:59:02.163725Z\",\"session_id\":\"72fc3f29-5d1e-4a24-8838-9446385e24bb\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"finished\",\"statement_id\":7,\"statement_ids\":[7]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5fb25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "üîÑ Downloading full CSV from '20-caracterizacao-pes-contrato-ativo' ...\n",
    "    ‚ö†Ô∏è Could not auto-detect delimiter. Falling back to ';'\n",
    "    ‚ÑπÔ∏è Detected separator: ';'\n",
    "    ‚úÖ File written to: abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/20_caracterizacao_pes_contrato_ativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4204a259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e9c926",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] Characterization of Consumption Points (CPEs), with active contracts (Dataset ID: 20-caracterizacao-pes-contrato-ativo). Pausing before next extraction...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801ffc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pause between dataset extraction commands ---\n",
    "    time_to_sleep = 60  # seconds (change if you want, e.g. 120 for extra caution)\n",
    "    for remaining in range(time_to_sleep, 0, -1):\n",
    "        print(f\"Sleeping for {remaining} seconds...\", end=\"\\r\")\n",
    "        time.sleep(1)\n",
    "    print(\"\\n[INFO] Sleep complete. Proceeding to next extraction.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707dcb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615b6e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-09T13:05:15.0114203Z\",\"execution_start_time\":\"2025-06-09T13:04:14.518803Z\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\",\"parent_msg_id\":\"8ffb088c-5b61-4fd7-b7ac-ce63ceff02bc\",\"queued_time\":\"2025-06-09T12:59:02.1651886Z\",\"session_id\":\"72fc3f29-5d1e-4a24-8838-9446385e24bb\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"finished\",\"statement_id\":8,\"statement_ids\":[8]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ab580e",
   "metadata": {},
   "outputs": [],
   "source": [
    "[INFO] Characterization of Consumption Points (CPEs), with active contracts (Dataset ID: 20-caracterizacao-pes-contrato-ativo). Pausing before next extraction...\n",
    "    Sleeping for 1 seconds....\n",
    "    [INFO] Sleep complete. Proceeding to next extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727ee8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "New connections to the network of power generation centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495a5fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset ID: 25_plr_producao_renovavel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7d3baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "    from datetime import datetime\n",
    "    from pyspark.sql import SparkSession\n",
    "    import pandas as pd\n",
    "    from io import BytesIO\n",
    "    import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64ea5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset name and ABFS path in the Lakehouse\n",
    "    DATASET_NAME = \"25-plr-producao-renovavel\"\n",
    "    ABFS_DIR = \"abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/25_plr_producao_renovavel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a098fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Download the full CSV from the API\n",
    "    def download_dataset():\n",
    "        url = f\"https://e-redes.opendatasoft.com/api/explore/v2.1/catalog/datasets/{DATASET_NAME}/exports/csv\"\n",
    "        print(f\"üîÑ Downloading full CSV from '{DATASET_NAME}' ...\")\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"‚ùå Failed to export: {response.status_code}\")\n",
    "        return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcf8af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Automatically detect CSV delimiter with fallback\n",
    "    def detect_separator(sample_text):\n",
    "        sniffer = csv.Sniffer()\n",
    "        try:\n",
    "            dialect = sniffer.sniff(sample_text)\n",
    "            return dialect.delimiter\n",
    "        except csv.Error:\n",
    "            print(\"‚ö†Ô∏è Could not auto-detect delimiter. Falling back to ';'\")\n",
    "            return \";\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b7836b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load with pandas, convert to Spark, and write to ABFS (overwrite)\n",
    "    def save_and_upload_to_abfs():\n",
    "        csv_bytes = download_dataset()\n",
    "        sample = csv_bytes.decode('utf-8')[:1000]\n",
    "        sep = detect_separator(sample)\n",
    "        print(f\"‚ÑπÔ∏è Detected separator: '{sep}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0f22f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load using pandas with detected or fallback separator\n",
    "        pdf = pd.read_csv(BytesIO(csv_bytes), sep=sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8aed17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Spark DataFrame\n",
    "        df = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee1f041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to ABFS, always overwriting the folder\n",
    "        df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(ABFS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c373dedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"‚úÖ File written to: {ABFS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc62d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "    save_and_upload_to_abfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2427c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-09T13:05:17.4901117Z\",\"execution_start_time\":\"2025-06-09T13:05:15.0133749Z\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\",\"parent_msg_id\":\"0bed2d39-2bf9-4124-a999-cfdbe0c8871b\",\"queued_time\":\"2025-06-09T12:59:02.1666392Z\",\"session_id\":\"72fc3f29-5d1e-4a24-8838-9446385e24bb\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"finished\",\"statement_id\":9,\"statement_ids\":[9]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0d9e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "üîÑ Downloading full CSV from '25-plr-producao-renovavel' ...\n",
    "    ‚ÑπÔ∏è Detected separator: ';'\n",
    "    ‚úÖ File written to: abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/25_plr_producao_renovavel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5520b5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1de1667",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] New connections to the network of power generation centers (Dataset ID: 25-plr-producao-renovavel). Pausing before next extraction...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cad6007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pause between dataset extraction commands ---\n",
    "    time_to_sleep = 60  # seconds (change if you want, e.g. 120 for extra caution)\n",
    "    for remaining in range(time_to_sleep, 0, -1):\n",
    "        print(f\"Sleeping for {remaining} seconds...\", end=\"\\r\")\n",
    "        time.sleep(1)\n",
    "    print(\"\\n[INFO] Sleep complete. Proceeding to next extraction.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad49fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f9f77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-09T13:06:18.72781Z\",\"execution_start_time\":\"2025-06-09T13:05:17.492347Z\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\",\"parent_msg_id\":\"ecca79f6-9648-4ff8-a345-3e0b11e3e781\",\"queued_time\":\"2025-06-09T12:59:02.1681772Z\",\"session_id\":\"72fc3f29-5d1e-4a24-8838-9446385e24bb\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"finished\",\"statement_id\":10,\"statement_ids\":[10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8154e85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "[INFO] New connections to the network of power generation centers (Dataset ID: 25-plr-producao-renovavel). Pausing before next extraction...\n",
    "    Sleeping for 1 seconds....\n",
    "    [INFO] Sleep complete. Proceeding to next extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaced06",
   "metadata": {},
   "outputs": [],
   "source": [
    "New production units for self-consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65f0e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset ID: 26_centrais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a5037e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "    from datetime import datetime\n",
    "    from pyspark.sql import SparkSession\n",
    "    import pandas as pd\n",
    "    from io import BytesIO\n",
    "    import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e835ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset name and ABFS path in the Lakehouse\n",
    "    DATASET_NAME = \"26-centrais\"\n",
    "    ABFS_DIR = \"abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/26_centrais\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2147bb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Download the full CSV from the API\n",
    "    def download_dataset():\n",
    "        url = f\"https://e-redes.opendatasoft.com/api/explore/v2.1/catalog/datasets/{DATASET_NAME}/exports/csv\"\n",
    "        print(f\"üîÑ Downloading full CSV from '{DATASET_NAME}' ...\")\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"‚ùå Failed to export: {response.status_code}\")\n",
    "        return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d76c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Automatically detect CSV delimiter with fallback\n",
    "    def detect_separator(sample_text):\n",
    "        sniffer = csv.Sniffer()\n",
    "        try:\n",
    "            dialect = sniffer.sniff(sample_text)\n",
    "            return dialect.delimiter\n",
    "        except csv.Error:\n",
    "            print(\"‚ö†Ô∏è Could not auto-detect delimiter. Falling back to ';'\")\n",
    "            return \";\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bffbc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load with pandas, convert to Spark, and write to ABFS (overwrite)\n",
    "    def save_and_upload_to_abfs():\n",
    "        csv_bytes = download_dataset()\n",
    "        sample = csv_bytes.decode('utf-8')[:1000]\n",
    "        sep = detect_separator(sample)\n",
    "        print(f\"‚ÑπÔ∏è Detected separator: '{sep}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f82fb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load using pandas with detected or fallback separator\n",
    "        pdf = pd.read_csv(BytesIO(csv_bytes), sep=sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdc7f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Spark DataFrame\n",
    "        df = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4a1499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to ABFS, always overwriting the folder\n",
    "        df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(ABFS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f84e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"‚úÖ File written to: {ABFS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11124bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "    save_and_upload_to_abfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72604523",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-09T13:06:22.2984048Z\",\"execution_start_time\":\"2025-06-09T13:06:18.729698Z\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\",\"parent_msg_id\":\"3254ae4a-d433-4e70-bfc2-e75c1e0a0ef6\",\"queued_time\":\"2025-06-09T12:59:02.1697287Z\",\"session_id\":\"72fc3f29-5d1e-4a24-8838-9446385e24bb\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"finished\",\"statement_id\":11,\"statement_ids\":[11]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc08f380",
   "metadata": {},
   "outputs": [],
   "source": [
    "üîÑ Downloading full CSV from '26-centrais' ...\n",
    "    ‚ÑπÔ∏è Detected separator: ';'\n",
    "    ‚úÖ File written to: abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/26_centrais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cf1179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29ff7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] New production units for self-consumption (Dataset ID: 26-centrais). Pausing before next extraction...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9314477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pause between dataset extraction commands ---\n",
    "    time_to_sleep = 60  # seconds (change if you want, e.g. 120 for extra caution)\n",
    "    for remaining in range(time_to_sleep, 0, -1):\n",
    "        print(f\"Sleeping for {remaining} seconds...\", end=\"\\r\")\n",
    "        time.sleep(1)\n",
    "    print(\"\\n[INFO] Sleep complete. Proceeding to next extraction.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812fb513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4e79ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-09T13:07:23.2893292Z\",\"execution_start_time\":\"2025-06-09T13:06:22.3006477Z\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\",\"parent_msg_id\":\"00cc9ecc-1e1c-4682-b111-6379f7e61d3a\",\"queued_time\":\"2025-06-09T12:59:02.1714045Z\",\"session_id\":\"72fc3f29-5d1e-4a24-8838-9446385e24bb\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"finished\",\"statement_id\":12,\"statement_ids\":[12]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6b4fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "[INFO] New production units for self-consumption (Dataset ID: 26-centrais). Pausing before next extraction...\n",
    "    Sleeping for 1 seconds....\n",
    "    [INFO] Sleep complete. Proceeding to next extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b94c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "New grid connections associated with electric mobility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4edf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset ID: 9_plr_mobilidade_eletrica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20bd070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "    from datetime import datetime\n",
    "    from pyspark.sql import SparkSession\n",
    "    import pandas as pd\n",
    "    from io import BytesIO\n",
    "    import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ed0f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset name and ABFS path in the Lakehouse\n",
    "    DATASET_NAME = \"9-plr-mobilidade-eletrica\"\n",
    "    ABFS_DIR = \"abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/9_plr_mobilidade_eletrica\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb5ae56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Download the full CSV from the API\n",
    "    def download_dataset():\n",
    "        url = f\"https://e-redes.opendatasoft.com/api/explore/v2.1/catalog/datasets/{DATASET_NAME}/exports/csv\"\n",
    "        print(f\"üîÑ Downloading full CSV from '{DATASET_NAME}' ...\")\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"‚ùå Failed to export: {response.status_code}\")\n",
    "        return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da0357a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Automatically detect CSV delimiter with fallback\n",
    "    def detect_separator(sample_text):\n",
    "        sniffer = csv.Sniffer()\n",
    "        try:\n",
    "            dialect = sniffer.sniff(sample_text)\n",
    "            return dialect.delimiter\n",
    "        except csv.Error:\n",
    "            print(\"‚ö†Ô∏è Could not auto-detect delimiter. Falling back to ';'\")\n",
    "            return \";\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5dd921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load with pandas, convert to Spark, and write to ABFS (overwrite)\n",
    "    def save_and_upload_to_abfs():\n",
    "        csv_bytes = download_dataset()\n",
    "        sample = csv_bytes.decode('utf-8')[:1000]\n",
    "        sep = detect_separator(sample)\n",
    "        print(f\"‚ÑπÔ∏è Detected separator: '{sep}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647fc2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load using pandas with detected or fallback separator\n",
    "        pdf = pd.read_csv(BytesIO(csv_bytes), sep=sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400c7524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Spark DataFrame\n",
    "        df = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02671400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to ABFS, always overwriting the folder\n",
    "        df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(ABFS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3948d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"‚úÖ File written to: {ABFS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2be0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "    save_and_upload_to_abfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3a619f",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-09T13:07:24.7325407Z\",\"execution_start_time\":\"2025-06-09T13:07:23.2915008Z\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\",\"parent_msg_id\":\"42b1e058-41e9-47e9-8c3e-83ef0ebd5cd9\",\"queued_time\":\"2025-06-09T12:59:02.1730756Z\",\"session_id\":\"72fc3f29-5d1e-4a24-8838-9446385e24bb\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"finished\",\"statement_id\":13,\"statement_ids\":[13]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb980068",
   "metadata": {},
   "outputs": [],
   "source": [
    "üîÑ Downloading full CSV from '9-plr-mobilidade-eletrica' ...\n",
    "    ‚ÑπÔ∏è Detected separator: ';'\n",
    "    ‚úÖ File written to: abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/9_plr_mobilidade_eletrica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc063b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e78f4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] New grid connections associated with electric mobility (Dataset ID: 9-plr-mobilidade-eletrica). Pausing before next extraction...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1396b30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pause between dataset extraction commands ---\n",
    "    time_to_sleep = 60  # seconds (change if you want, e.g. 120 for extra caution)\n",
    "    for remaining in range(time_to_sleep, 0, -1):\n",
    "        print(f\"Sleeping for {remaining} seconds...\", end=\"\\r\")\n",
    "        time.sleep(1)\n",
    "    print(\"\\n[INFO] Sleep complete. Proceeding to next extraction.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daae7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d12ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-09T13:08:26.6404444Z\",\"execution_start_time\":\"2025-06-09T13:07:24.7347305Z\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\",\"parent_msg_id\":\"5017a8b8-a20c-4a76-8114-49b52e689f90\",\"queued_time\":\"2025-06-09T12:59:02.174632Z\",\"session_id\":\"72fc3f29-5d1e-4a24-8838-9446385e24bb\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"finished\",\"statement_id\":14,\"statement_ids\":[14]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49a7895",
   "metadata": {},
   "outputs": [],
   "source": [
    "[INFO] New grid connections associated with electric mobility (Dataset ID: 9-plr-mobilidade-eletrica). Pausing before next extraction...\n",
    "    Sleeping for 1 seconds....\n",
    "    [INFO] Sleep complete. Proceeding to next extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77572197",
   "metadata": {},
   "outputs": [],
   "source": [
    "Number of low voltage delivery points with collection of Load Diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b532206",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset ID: 22_diagrama_de_carga_por_instalacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0759c616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "    from datetime import datetime\n",
    "    from pyspark.sql import SparkSession\n",
    "    import pandas as pd\n",
    "    from io import BytesIO\n",
    "    import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd0e658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset name and ABFS path in the Lakehouse\n",
    "    DATASET_NAME = \"22-diagrama-de-carga-por-instalacao\"\n",
    "    ABFS_DIR = \"abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/22_diagrama_de_carga_por_instalacao\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10f9c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Download the full CSV from the API\n",
    "    def download_dataset():\n",
    "        url = f\"https://e-redes.opendatasoft.com/api/explore/v2.1/catalog/datasets/{DATASET_NAME}/exports/csv\"\n",
    "        print(f\"üîÑ Downloading full CSV from '{DATASET_NAME}' ...\")\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"‚ùå Failed to export: {response.status_code}\")\n",
    "        return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45de1040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Automatically detect CSV delimiter with fallback\n",
    "    def detect_separator(sample_text):\n",
    "        sniffer = csv.Sniffer()\n",
    "        try:\n",
    "            dialect = sniffer.sniff(sample_text)\n",
    "            return dialect.delimiter\n",
    "        except csv.Error:\n",
    "            print(\"‚ö†Ô∏è Could not auto-detect delimiter. Falling back to ';'\")\n",
    "            return \";\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59810593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load with pandas, convert to Spark, and write to ABFS (overwrite)\n",
    "    def save_and_upload_to_abfs():\n",
    "        csv_bytes = download_dataset()\n",
    "        sample = csv_bytes.decode('utf-8')[:1000]\n",
    "        sep = detect_separator(sample)\n",
    "        print(f\"‚ÑπÔ∏è Detected separator: '{sep}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f15d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load using pandas with detected or fallback separator\n",
    "        pdf = pd.read_csv(BytesIO(csv_bytes), sep=sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b89edb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Spark DataFrame\n",
    "        df = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de116dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to ABFS, always overwriting the folder\n",
    "        df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(ABFS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cab55de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"‚úÖ File written to: {ABFS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e496a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "    save_and_upload_to_abfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7718d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-09T13:08:44.2728616Z\",\"execution_start_time\":\"2025-06-09T13:08:26.6426691Z\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\",\"parent_msg_id\":\"be832f6f-4ecb-44f3-a8db-f188933a67ec\",\"queued_time\":\"2025-06-09T12:59:02.1762604Z\",\"session_id\":\"72fc3f29-5d1e-4a24-8838-9446385e24bb\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"finished\",\"statement_id\":15,\"statement_ids\":[15]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a51118",
   "metadata": {},
   "outputs": [],
   "source": [
    "üîÑ Downloading full CSV from '22-diagrama-de-carga-por-instalacao' ...\n",
    "    ‚ÑπÔ∏è Detected separator: ';'\n",
    "    ‚úÖ File written to: abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/22_diagrama_de_carga_por_instalacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e5d4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9e3061",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] Number of low voltage delivery points with collection of Load Diagrams (Dataset ID: 22-diagrama-de-carga-por-instalacao). Pausing before next extraction...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f583bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pause between dataset extraction commands ---\n",
    "    time_to_sleep = 60  # seconds (change if you want, e.g. 120 for extra caution)\n",
    "    for remaining in range(time_to_sleep, 0, -1):\n",
    "        print(f\"Sleeping for {remaining} seconds...\", end=\"\\r\")\n",
    "        time.sleep(1)\n",
    "    print(\"\\n[INFO] Sleep complete. Proceeding to next extraction.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa605e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6bee82",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-09T13:09:44.5974169Z\",\"execution_start_time\":\"2025-06-09T13:08:44.2747927Z\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\",\"parent_msg_id\":\"91527b0b-b218-4c18-9961-87c21a0e2434\",\"queued_time\":\"2025-06-09T12:59:02.1777396Z\",\"session_id\":\"72fc3f29-5d1e-4a24-8838-9446385e24bb\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"finished\",\"statement_id\":16,\"statement_ids\":[16]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e67725",
   "metadata": {},
   "outputs": [],
   "source": [
    "[INFO] Number of low voltage delivery points with collection of Load Diagrams (Dataset ID: 22-diagrama-de-carga-por-instalacao). Pausing before next extraction...\n",
    "    Sleeping for 1 seconds....\n",
    "    [INFO] Sleep complete. Proceeding to next extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fba3ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Connection points for Electric Vehicle Charging Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c67bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset ID: postos_carregamento_ves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a916eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "    from datetime import datetime\n",
    "    from pyspark.sql import SparkSession\n",
    "    import pandas as pd\n",
    "    from io import BytesIO\n",
    "    import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd18b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset name and ABFS path in the Lakehouse\n",
    "    DATASET_NAME = \"postos_carregamento_ves\"\n",
    "    ABFS_DIR = \"abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/postos_carregamento_ves\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb53e8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Download the full CSV from the API\n",
    "    def download_dataset():\n",
    "        url = f\"https://e-redes.opendatasoft.com/api/explore/v2.1/catalog/datasets/{DATASET_NAME}/exports/csv\"\n",
    "        print(f\"üîÑ Downloading full CSV from '{DATASET_NAME}' ...\")\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"‚ùå Failed to export: {response.status_code}\")\n",
    "        return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37581fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Automatically detect CSV delimiter with fallback\n",
    "    def detect_separator(sample_text):\n",
    "        sniffer = csv.Sniffer()\n",
    "        try:\n",
    "            dialect = sniffer.sniff(sample_text)\n",
    "            return dialect.delimiter\n",
    "        except csv.Error:\n",
    "            print(\"‚ö†Ô∏è Could not auto-detect delimiter. Falling back to ';'\")\n",
    "            return \";\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291cdac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load with pandas, convert to Spark, and write to ABFS (overwrite)\n",
    "    def save_and_upload_to_abfs():\n",
    "        csv_bytes = download_dataset()\n",
    "        sample = csv_bytes.decode('utf-8')[:1000]\n",
    "        sep = detect_separator(sample)\n",
    "        print(f\"‚ÑπÔ∏è Detected separator: '{sep}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348dc4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load using pandas with detected or fallback separator\n",
    "        pdf = pd.read_csv(BytesIO(csv_bytes), sep=sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e3baee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Spark DataFrame\n",
    "        df = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be813654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to ABFS, always overwriting the folder\n",
    "        df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(ABFS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0b47ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"‚úÖ File written to: {ABFS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf71283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "    save_and_upload_to_abfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8838cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-09T13:09:52.3889438Z\",\"execution_start_time\":\"2025-06-09T13:09:44.5993941Z\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\",\"parent_msg_id\":\"e9f2691f-2905-46e5-829d-eb3ae110f23d\",\"queued_time\":\"2025-06-09T12:59:04.7785583Z\",\"session_id\":\"72fc3f29-5d1e-4a24-8838-9446385e24bb\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"finished\",\"statement_id\":17,\"statement_ids\":[17]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1dd21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "üîÑ Downloading full CSV from 'postos_carregamento_ves' ...\n",
    "    ‚ÑπÔ∏è Detected separator: ';'\n",
    "    ‚úÖ File written to: abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/postos_carregamento_ves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e8066b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60502859",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] Connection points for Electric Vehicle Charging Stations (Dataset ID: postos_carregamento_ves). Pausing before next extraction...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4263324b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pause between dataset extraction commands ---\n",
    "    time_to_sleep = 60  # seconds (change if you want, e.g. 120 for extra caution)\n",
    "    for remaining in range(time_to_sleep, 0, -1):\n",
    "        print(f\"Sleeping for {remaining} seconds...\", end=\"\\r\")\n",
    "        time.sleep(1)\n",
    "    print(\"\\n[INFO] Sleep complete. Proceeding to next extraction.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d346fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdb90ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-09T13:10:54.5648867Z\",\"execution_start_time\":\"2025-06-09T13:09:52.390927Z\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\",\"parent_msg_id\":\"c88910e7-cc73-483c-95d7-e638ded4bdad\",\"queued_time\":\"2025-06-09T12:59:05.6840112Z\",\"session_id\":\"72fc3f29-5d1e-4a24-8838-9446385e24bb\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"finished\",\"statement_id\":18,\"statement_ids\":[18]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fa95eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "[INFO] Connection points for Electric Vehicle Charging Stations (Dataset ID: postos_carregamento_ves). Pausing before next extraction...\n",
    "    Sleeping for 1 seconds....\n",
    "    [INFO] Sleep complete. Proceeding to next extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b85d522",
   "metadata": {},
   "outputs": [],
   "source": [
    "Network connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0b3a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset ID: 16_pedidos_concluidos_plrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f26e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "    from datetime import datetime\n",
    "    from pyspark.sql import SparkSession\n",
    "    import pandas as pd\n",
    "    from io import BytesIO\n",
    "    import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a08d72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset name and ABFS path in the Lakehouse\n",
    "    DATASET_NAME = \"16-pedidos-concluidos-plrs\"\n",
    "    ABFS_DIR = \"abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/16_pedidos_concluidos_plrs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42d4741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Download the full CSV from the API\n",
    "    def download_dataset():\n",
    "        url = f\"https://e-redes.opendatasoft.com/api/explore/v2.1/catalog/datasets/{DATASET_NAME}/exports/csv\"\n",
    "        print(f\"üîÑ Downloading full CSV from '{DATASET_NAME}' ...\")\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"‚ùå Failed to export: {response.status_code}\")\n",
    "        return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c3e8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Automatically detect CSV delimiter with fallback\n",
    "    def detect_separator(sample_text):\n",
    "        sniffer = csv.Sniffer()\n",
    "        try:\n",
    "            dialect = sniffer.sniff(sample_text)\n",
    "            return dialect.delimiter\n",
    "        except csv.Error:\n",
    "            print(\"‚ö†Ô∏è Could not auto-detect delimiter. Falling back to ';'\")\n",
    "            return \";\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118fd368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load with pandas, convert to Spark, and write to ABFS (overwrite)\n",
    "    def save_and_upload_to_abfs():\n",
    "        csv_bytes = download_dataset()\n",
    "        sample = csv_bytes.decode('utf-8')[:1000]\n",
    "        sep = detect_separator(sample)\n",
    "        print(f\"‚ÑπÔ∏è Detected separator: '{sep}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd80be75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load using pandas with detected or fallback separator\n",
    "        pdf = pd.read_csv(BytesIO(csv_bytes), sep=sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4299a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Spark DataFrame\n",
    "        df = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d87ca9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to ABFS, always overwriting the folder\n",
    "        df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(ABFS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0ca4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"‚úÖ File written to: {ABFS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ec3af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "    save_and_upload_to_abfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7555c7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-09T13:10:57.9726287Z\",\"execution_start_time\":\"2025-06-09T13:10:54.5668561Z\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\",\"parent_msg_id\":\"a48d1b21-2ad3-4c5c-ad45-4ed67095a131\",\"queued_time\":\"2025-06-09T12:59:06.0140927Z\",\"session_id\":\"72fc3f29-5d1e-4a24-8838-9446385e24bb\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"finished\",\"statement_id\":19,\"statement_ids\":[19]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5c5ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "üîÑ Downloading full CSV from '16-pedidos-concluidos-plrs' ...\n",
    "    ‚ÑπÔ∏è Detected separator: ';'\n",
    "    ‚úÖ File written to: abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/16_pedidos_concluidos_plrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f9aab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ace9848",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] Network connections (Dataset ID: 16-pedidos-concluidos-plrs). Pausing before next extraction...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d65fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pause between dataset extraction commands ---\n",
    "    time_to_sleep = 60  # seconds (change if you want, e.g. 120 for extra caution)\n",
    "    for remaining in range(time_to_sleep, 0, -1):\n",
    "        print(f\"Sleeping for {remaining} seconds...\", end=\"\\r\")\n",
    "        time.sleep(1)\n",
    "    print(\"\\n[INFO] Sleep complete. Proceeding to next extraction.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eb8cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d108ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-09T13:11:58.6904631Z\",\"execution_start_time\":\"2025-06-09T13:10:57.974525Z\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\",\"parent_msg_id\":\"6daa3842-4182-47e5-855f-057c272e4124\",\"queued_time\":\"2025-06-09T12:59:06.2922361Z\",\"session_id\":\"72fc3f29-5d1e-4a24-8838-9446385e24bb\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"finished\",\"statement_id\":20,\"statement_ids\":[20]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4716246d",
   "metadata": {},
   "outputs": [],
   "source": [
    "[INFO] Network connections (Dataset ID: 16-pedidos-concluidos-plrs). Pausing before next extraction...\n",
    "    Sleeping for 1 seconds....\n",
    "    [INFO] Sleep complete. Proceeding to next extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637e9c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hourly Consumption by Postal Code - 7 Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24961cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset ID: consumo_horario_codigo_postal_7_digitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076fcffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "    from pyspark.sql import SparkSession\n",
    "    import pandas as pd\n",
    "    from io import BytesIO\n",
    "    import csv\n",
    "    import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bd0cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subdatasets (por range de c√≥digo postal)\n",
    "    SUBDATASETS = [\n",
    "        \"consumoshorariocodigopostal1000a2000\",\n",
    "        \"consumoshorariocodigopostal2001a2500\",\n",
    "        \"consumoshorariocodigopostal2821a3080\",\n",
    "        \"consumoshorariocodigopostal5201a7400\",\n",
    "        \"consumoshorariocodigopostal2501a2695\",\n",
    "        \"consumoshorariocodigopostal2696a2820\",\n",
    "        \"consumoshorariocodigopostal3081a3780\",\n",
    "        \"consumoshorariocodigopostal4551a4770\",\n",
    "        \"consumoshorariocodigopostal7401a8970\",\n",
    "        \"consumoshorariocodigopostal3781a4420\",\n",
    "        \"consumoshorariocodigopostal4421a4550\",\n",
    "        \"consumoshorariocodigopostal4771a5200\",\n",
    "        \"consumoshorariocodigopostaloutros\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18da12db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base ABFS path (sem coalesce por defeito)\n",
    "    BASE_ABFS_PATH = \"abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/hourly_consumption\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1e0c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d417c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-detecta delimitador (usado pelo pandas)\n",
    "    def detect_separator(sample_text):\n",
    "        sniffer = csv.Sniffer()\n",
    "        try:\n",
    "            dialect = sniffer.sniff(sample_text)\n",
    "            return dialect.delimiter\n",
    "        except csv.Error:\n",
    "            print(\"‚ö†Ô∏è Could not auto-detect delimiter. Falling back to ';'\")\n",
    "            return \";\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16fe8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o principal para processar cada subdataset\n",
    "    def process_subdataset(dataset_id):\n",
    "        try:\n",
    "            print(f\"\\nüîÑ Downloading CSV from '{dataset_id}' ...\")\n",
    "            url = f\"https://e-redes.opendatasoft.com/api/explore/v2.1/catalog/datasets/{dataset_id}/exports/csv\"\n",
    "            response = requests.get(url)\n",
    "            if response.status_code != 200:\n",
    "                raise Exception(f\"‚ùå Failed to export {dataset_id}: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f048106",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_bytes = response.content\n",
    "            sample = csv_bytes.decode('utf-8')[:1000]\n",
    "            sep = detect_separator(sample)\n",
    "            print(f\"‚ÑπÔ∏è Detected separator for {dataset_id}: '{sep}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffe6391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas ‚Üí Spark\n",
    "            pdf = pd.read_csv(BytesIO(csv_bytes), sep=sep)\n",
    "            df = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2c8431",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_count = df.count()\n",
    "            print(f\"‚ÑπÔ∏è Rows in {dataset_id}: {row_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f373aef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminho final\n",
    "            abfs_path = os.path.join(BASE_ABFS_PATH, dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e67670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se for pequeno, pode gravar com coalesce(1)\n",
    "            if row_count <= 100_000:\n",
    "                df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(abfs_path)\n",
    "                print(f\"‚úÖ Saved (coalesced) to: {abfs_path}\")\n",
    "            else:\n",
    "                df.write.mode(\"overwrite\").option(\"header\", True).csv(abfs_path)\n",
    "                print(f\"‚úÖ Saved (parallel) to: {abfs_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722d1edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "except Exception as e:\n",
    "            print(f\"‚ùå Error processing {dataset_id}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c53c8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop geral\n",
    "    for sub_id in SUBDATASETS:\n",
    "        process_subdataset(sub_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09004627",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":null,\"execution_start_time\":\"2025-06-11T09:48:54.703032Z\",\"livy_statement_state\":\"running\",\"normalized_state\":\"running\",\"parent_msg_id\":\"4e4eb746-e8be-4050-87da-fc7033d8f061\",\"queued_time\":\"2025-06-11T09:48:48.3170657Z\",\"session_id\":\"3bb56de1-d08e-42c8-9d9e-b81c838a9b2b\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"submitted\",\"statement_id\":4,\"statement_ids\":[4]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32597ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "üîÑ Downloading CSV from 'consumoshorariocodigopostal1000a2000' ...\n",
    "    ‚ÑπÔ∏è Detected separator for consumoshorariocodigopostal1000a2000: ';'\n",
    "    ‚ÑπÔ∏è Rows in consumoshorariocodigopostal1000a2000: 3886862\n",
    "    ‚úÖ Saved (parallel) to: abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/hourly_consumption/consumoshorariocodigopostal1000a2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3343888c",
   "metadata": {},
   "outputs": [],
   "source": [
    "üîÑ Downloading CSV from 'consumoshorariocodigopostal2001a2500' ...\n",
    "    ‚ÑπÔ∏è Detected separator for consumoshorariocodigopostal2001a2500: ';'\n",
    "    ‚ÑπÔ∏è Rows in consumoshorariocodigopostal2001a2500: 3863214\n",
    "    ‚úÖ Saved (parallel) to: abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/hourly_consumption/consumoshorariocodigopostal2001a2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc8343c",
   "metadata": {},
   "outputs": [],
   "source": [
    "üîÑ Downloading CSV from 'consumoshorariocodigopostal2821a3080' ...\n",
    "    ‚ÑπÔ∏è Detected separator for consumoshorariocodigopostal2821a3080: ';'\n",
    "    ‚ÑπÔ∏è Rows in consumoshorariocodigopostal2821a3080: 4211342\n",
    "    ‚úÖ Saved (parallel) to: abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/hourly_consumption/consumoshorariocodigopostal2821a3080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd86ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "üîÑ Downloading CSV from 'consumoshorariocodigopostal5201a7400' ...\n",
    "    ‚ÑπÔ∏è Detected separator for consumoshorariocodigopostal5201a7400: ';'\n",
    "    ‚ÑπÔ∏è Rows in consumoshorariocodigopostal5201a7400: 4080405\n",
    "    ‚úÖ Saved (parallel) to: abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/hourly_consumption/consumoshorariocodigopostal5201a7400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdc4637",
   "metadata": {},
   "outputs": [],
   "source": [
    "üîÑ Downloading CSV from 'consumoshorariocodigopostal2501a2695' ...\n",
    "    ‚ÑπÔ∏è Detected separator for consumoshorariocodigopostal2501a2695: ';'\n",
    "    ‚ÑπÔ∏è Rows in consumoshorariocodigopostal2501a2695: 4098601\n",
    "    ‚úÖ Saved (parallel) to: abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/hourly_consumption/consumoshorariocodigopostal2501a2695"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b93f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "üîÑ Downloading CSV from 'consumoshorariocodigopostal2696a2820' ...\n",
    "    ‚ÑπÔ∏è Detected separator for consumoshorariocodigopostal2696a2820: ';'\n",
    "    ‚ÑπÔ∏è Rows in consumoshorariocodigopostal2696a2820: 4203581\n",
    "    ‚úÖ Saved (parallel) to: abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/hourly_consumption/consumoshorariocodigopostal2696a2820"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf428bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "üîÑ Downloading CSV from 'consumoshorariocodigopostal3081a3780' ...\n",
    "    ‚ÑπÔ∏è Detected separator for consumoshorariocodigopostal3081a3780: ';'\n",
    "    ‚ÑπÔ∏è Rows in consumoshorariocodigopostal3081a3780: 4087201\n",
    "    ‚úÖ Saved (parallel) to: abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/hourly_consumption/consumoshorariocodigopostal3081a3780"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944aaedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "üîÑ Downloading CSV from 'consumoshorariocodigopostal4551a4770' ...\n",
    "    ‚ÑπÔ∏è Detected separator for consumoshorariocodigopostal4551a4770: ';'\n",
    "    ‚ÑπÔ∏è Rows in consumoshorariocodigopostal4551a4770: 4128058\n",
    "    ‚úÖ Saved (parallel) to: abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/hourly_consumption/consumoshorariocodigopostal4551a4770"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f34054",
   "metadata": {},
   "outputs": [],
   "source": [
    "üîÑ Downloading CSV from 'consumoshorariocodigopostal7401a8970' ...\n",
    "    ‚ÑπÔ∏è Detected separator for consumoshorariocodigopostal7401a8970: ';'\n",
    "    ‚ÑπÔ∏è Rows in consumoshorariocodigopostal7401a8970: 3718788\n",
    "    ‚úÖ Saved (parallel) to: abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/hourly_consumption/consumoshorariocodigopostal7401a8970"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3b42c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "üîÑ Downloading CSV from 'consumoshorariocodigopostal3781a4420' ...\n",
    "    ‚ÑπÔ∏è Detected separator for consumoshorariocodigopostal3781a4420: ';'\n",
    "    ‚ÑπÔ∏è Rows in consumoshorariocodigopostal3781a4420: 4082326\n",
    "    ‚úÖ Saved (parallel) to: abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/hourly_consumption/consumoshorariocodigopostal3781a4420"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9025acce",
   "metadata": {},
   "outputs": [],
   "source": [
    "üîÑ Downloading CSV from 'consumoshorariocodigopostal4421a4550' ...\n",
    "    ‚ÑπÔ∏è Detected separator for consumoshorariocodigopostal4421a4550: ';'\n",
    "    ‚ÑπÔ∏è Rows in consumoshorariocodigopostal4421a4550: 4232202\n",
    "    ‚úÖ Saved (parallel) to: abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/hourly_consumption/consumoshorariocodigopostal4421a4550"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930463a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "üîÑ Downloading CSV from 'consumoshorariocodigopostal4771a5200' ...\n",
    "    ‚ÑπÔ∏è Detected separator for consumoshorariocodigopostal4771a5200: ';'\n",
    "    ‚ÑπÔ∏è Rows in consumoshorariocodigopostal4771a5200: 4088805\n",
    "    ‚úÖ Saved (parallel) to: abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/hourly_consumption/consumoshorariocodigopostal4771a5200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f65ca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "üîÑ Downloading CSV from 'consumoshorariocodigopostaloutros' ...\n",
    "    ‚ÑπÔ∏è Detected separator for consumoshorariocodigopostaloutros: ';'\n",
    "    ‚ÑπÔ∏è Rows in consumoshorariocodigopostaloutros: 696\n",
    "    ‚úÖ Saved (coalesced) to: abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/hourly_consumption/consumoshorariocodigopostaloutros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06466bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab14edf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] Hourly Consumption by Postal Code - 7 Digits (Dataset ID: consumo_horario_codigo_postal_7_digitos). Pausing before next extraction...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780655f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pause between dataset extraction commands ---\n",
    "    time_to_sleep = 60  # seconds (change if you want, e.g. 120 for extra caution)\n",
    "    for remaining in range(time_to_sleep, 0, -1):\n",
    "        print(f\"Sleeping for {remaining} seconds...\", end=\"\\r\")\n",
    "        time.sleep(1)\n",
    "    print(\"\\n[INFO] Sleep complete. Proceeding to next extraction.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28de73ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82e2c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-09T13:16:46.4686489Z\",\"execution_start_time\":\"2025-06-09T13:15:44.9438504Z\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\",\"parent_msg_id\":\"31f8637c-9da3-4427-a113-9f844428d710\",\"queued_time\":\"2025-06-09T12:59:06.6638425Z\",\"session_id\":\"72fc3f29-5d1e-4a24-8838-9446385e24bb\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"finished\",\"statement_id\":22,\"statement_ids\":[22]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960681d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "[INFO] Hourly Consumption by Postal Code - 7 Digits (Dataset ID: consumo_horario_codigo_postal_7_digitos). Pausing before next extraction...\n",
    "    Sleeping for 1 seconds....\n",
    "    [INFO] Sleep complete. Proceeding to next extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373f3df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hourly Consumption by Postal Code - 4 Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94ff6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset ID: consumos_horario_codigo_postal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce9a582",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "    from datetime import datetime\n",
    "    from pyspark.sql import SparkSession\n",
    "    import pandas as pd\n",
    "    from io import BytesIO\n",
    "    import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0853561d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset name and ABFS path in the Lakehouse\n",
    "    DATASET_NAME = \"consumos_horario_codigo_postal\"\n",
    "    ABFS_DIR = \"abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/consumos_horario_codigo_postal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8684810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Download the full CSV from the API\n",
    "    def download_dataset():\n",
    "        url = f\"https://e-redes.opendatasoft.com/api/explore/v2.1/catalog/datasets/{DATASET_NAME}/exports/csv\"\n",
    "        print(f\"üîÑ Downloading full CSV from '{DATASET_NAME}' ...\")\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"‚ùå Failed to export: {response.status_code}\")\n",
    "        return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b737d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Automatically detect CSV delimiter with fallback\n",
    "    def detect_separator(sample_text):\n",
    "        sniffer = csv.Sniffer()\n",
    "        try:\n",
    "            dialect = sniffer.sniff(sample_text)\n",
    "            return dialect.delimiter\n",
    "        except csv.Error:\n",
    "            print(\"‚ö†Ô∏è Could not auto-detect delimiter. Falling back to ';'\")\n",
    "            return \";\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1add43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load with pandas, convert to Spark, and write to ABFS (overwrite)\n",
    "    def save_and_upload_to_abfs():\n",
    "        csv_bytes = download_dataset()\n",
    "        sample = csv_bytes.decode('utf-8')[:1000]\n",
    "        sep = detect_separator(sample)\n",
    "        print(f\"‚ÑπÔ∏è Detected separator: '{sep}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510accac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load using pandas with detected or fallback separator\n",
    "        pdf = pd.read_csv(BytesIO(csv_bytes), sep=sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b231da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Spark DataFrame\n",
    "        df = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a6ba74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to ABFS, always overwriting the folder\n",
    "        df.write.mode(\"overwrite\").option(\"header\", True).csv(ABFS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd7f161",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"‚úÖ File written to: {ABFS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1261381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "    save_and_upload_to_abfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71460947",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-11T14:35:18.507392Z\",\"execution_start_time\":\"2025-06-11T14:20:07.1122199Z\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\",\"parent_msg_id\":\"d4797cab-5ddf-40f0-b72e-50b762fac8ae\",\"queued_time\":\"2025-06-11T14:20:07.1109417Z\",\"session_id\":\"3bb56de1-d08e-42c8-9d9e-b81c838a9b2b\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"finished\",\"statement_id\":6,\"statement_ids\":[6]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5447500d",
   "metadata": {},
   "outputs": [],
   "source": [
    "üîÑ Downloading full CSV from 'consumos_horario_codigo_postal' ...\n",
    "    ‚ÑπÔ∏è Detected separator: ';'\n",
    "    ‚úÖ File written to: abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/consumos_horario_codigo_postal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1bc8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cd9b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] HHourly Consumption by Postal Code - 4 Digits (Dataset ID: consumos_horario_codigo_postal). Pausing before next extraction...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf2c952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pause between dataset extraction commands ---\n",
    "    time_to_sleep = 60  # seconds (change if you want, e.g. 120 for extra caution)\n",
    "    for remaining in range(time_to_sleep, 0, -1):\n",
    "        print(f\"Sleeping for {remaining} seconds...\", end=\"\\r\")\n",
    "        time.sleep(1)\n",
    "    print(\"\\n[INFO] Sleep complete. Proceeding to next extraction.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712cf3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d906ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-09T13:17:02.9209307Z\",\"execution_start_time\":null,\"livy_statement_state\":null,\"normalized_state\":\"cancelled\",\"parent_msg_id\":\"3e105962-e9d1-4418-a767-e263eed01e4d\",\"queued_time\":\"2025-06-09T12:59:07.0872355Z\",\"session_id\":\"72fc3f29-5d1e-4a24-8838-9446385e24bb\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"cancelled\",\"statement_id\":-1,\"statement_ids\":null}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1554f74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Low Voltage Poles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3274979c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset ID: apoios-baixa-tensao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2632a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7653ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ID = \"apoios-baixa-tensao\"\n",
    "    BASE_PATH = f\"/lakehouse/default/Files/eredes/{DATASET_ID}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fe57ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_paged(offset, limit=10000):\n",
    "        url = f\"https://e-redes.opendatasoft.com/api/explore/v2.1/catalog/datasets/{DATASET_ID}/records\"\n",
    "        params = {\n",
    "            \"limit\": limit,\n",
    "            \"offset\": offset\n",
    "        }\n",
    "        while True:\n",
    "            resp = requests.get(url, params=params)\n",
    "            if resp.status_code == 429:\n",
    "                print(f\"[{DATASET_ID}] [429] Rate limit hit at offset {offset}. Sleeping 30 minutes and retrying...\")\n",
    "                time.sleep(1800)\n",
    "                continue\n",
    "            if resp.status_code != 200:\n",
    "                print(f\"[{DATASET_ID}] Error at offset {offset}: {resp.status_code}\")\n",
    "                return []\n",
    "            break\n",
    "        return resp.json().get(\"results\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb3970c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_chunk(data, chunk_idx):\n",
    "        if not data:\n",
    "            print(f\"  No data in chunk {chunk_idx}, file NOT created.\")\n",
    "            return\n",
    "        os.makedirs(BASE_PATH, exist_ok=True)\n",
    "        df = pd.DataFrame(data)\n",
    "        output_path = f\"{BASE_PATH}/{DATASET_ID}_chunk_{chunk_idx:04d}.csv\"\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"  File created: {output_path} ({len(df)} records)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787950f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MAIN SCRIPT ===\n",
    "    LIMIT = 10000  # podes mudar para 5000, 2000, etc\n",
    "    total = 2996997  # total de registos (confirma no in√≠cio, pode mudar!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3098a9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_idx = 0\n",
    "    for offset in range(0, total, LIMIT):\n",
    "        print(f\"[{DATASET_ID}] Downloading records {offset} to {offset + LIMIT - 1} ...\")\n",
    "        data = fetch_paged(offset, LIMIT)\n",
    "        if not data:\n",
    "            print(f\"  Stopped at chunk {chunk_idx}.\")\n",
    "            break\n",
    "        save_chunk(data, chunk_idx)\n",
    "        chunk_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3993e933",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[ALL DONE!]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e636556",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-09T13:17:02.9212246Z\",\"execution_start_time\":null,\"livy_statement_state\":null,\"normalized_state\":\"cancelled\",\"parent_msg_id\":\"ee6c9630-7f16-451c-b243-5e0cf477a28e\",\"queued_time\":\"2025-06-09T12:59:07.2932754Z\",\"session_id\":\"72fc3f29-5d1e-4a24-8838-9446385e24bb\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"cancelled\",\"statement_id\":-1,\"statement_ids\":null}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68c083d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e2335f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] Low Voltage Poles (Dataset ID: apoios-baixa-tensao). Pausing before next extraction...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8190c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pause between dataset extraction commands ---\n",
    "    time_to_sleep = 60  # seconds (change if you want, e.g. 120 for extra caution)\n",
    "    for remaining in range(time_to_sleep, 0, -1):\n",
    "        print(f\"Sleeping for {remaining} seconds...\", end=\"\\r\")\n",
    "        time.sleep(1)\n",
    "    print(\"\\n[INFO] Sleep complete. Proceeding to next extraction.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b792a357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611c353c",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-09T13:17:02.9214953Z\",\"execution_start_time\":null,\"livy_statement_state\":null,\"normalized_state\":\"cancelled\",\"parent_msg_id\":\"51712a8d-4059-4733-8b47-614188714f2c\",\"queued_time\":\"2025-06-09T12:59:07.5111682Z\",\"session_id\":\"72fc3f29-5d1e-4a24-8838-9446385e24bb\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"cancelled\",\"statement_id\":-1,\"statement_ids\":null}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f5bd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Secondary Substations (PTD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e41fb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset ID: postos-transformacao-distribuicao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6e607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad27b502",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ID = \"postos-transformacao-distribuicao\"\n",
    "    BASE_PATH = f\"/lakehouse/default/Files/eredes/{DATASET_ID}\"\n",
    "    LIMIT = 100\n",
    "    CHUNK_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07be356d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_paged(offset, limit=LIMIT):\n",
    "        url = f\"https://e-redes.opendatasoft.com/api/explore/v2.1/catalog/datasets/{DATASET_ID}/records\"\n",
    "        params = {\n",
    "            \"limit\": limit,\n",
    "            \"offset\": offset\n",
    "        }\n",
    "        while True:\n",
    "            resp = requests.get(url, params=params)\n",
    "            if resp.status_code == 429:\n",
    "                print(f\"[{DATASET_ID}] [429] Rate limit hit at offset {offset}. Sleeping 30 minutes before retrying...\")\n",
    "                time.sleep(1800)\n",
    "                continue\n",
    "            if resp.status_code != 200:\n",
    "                print(f\"[{DATASET_ID}] Error at offset {offset}: {resp.status_code}\")\n",
    "                return []\n",
    "            break\n",
    "        return resp.json().get(\"results\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6cb10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(BASE_PATH, exist_ok=True)\n",
    "    all_data = []\n",
    "    chunk_idx = 0\n",
    "    offset = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23bacac",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "        print(f\"[{DATASET_ID}] Downloading records {offset} to {offset + LIMIT - 1} ...\")\n",
    "        data = fetch_paged(offset, LIMIT)\n",
    "        if not data:\n",
    "            print(f\"  No more data at offset {offset}. Stopping.\")\n",
    "            break\n",
    "        all_data.extend(data)\n",
    "        if len(all_data) >= CHUNK_SIZE:\n",
    "            df = pd.DataFrame(all_data)\n",
    "            output_path = f\"{BASE_PATH}/{DATASET_ID}_chunk_{chunk_idx:04d}.csv\"\n",
    "            df.to_csv(output_path, index=False)\n",
    "            print(f\"  File created: {output_path} ({len(df)} records)\")\n",
    "            all_data = []\n",
    "            chunk_idx += 1\n",
    "        # Se o n√∫mero de registos recebidos for menor do que o limite, terminaste\n",
    "        if len(data) < LIMIT:\n",
    "            break\n",
    "        offset += LIMIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3909e3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar o que resta\n",
    "    if all_data:\n",
    "        df = pd.DataFrame(all_data)\n",
    "        output_path = f\"{BASE_PATH}/{DATASET_ID}_chunk_{chunk_idx:04d}.csv\"\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"  File created: {output_path} ({len(df)} records)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a89c788",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[ALL DONE!]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ef1ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-09T13:17:02.9217496Z\",\"execution_start_time\":null,\"livy_statement_state\":null,\"normalized_state\":\"cancelled\",\"parent_msg_id\":\"a672e6f8-f406-45ce-a1b9-badf2a9c5d29\",\"queued_time\":\"2025-06-09T12:59:07.6684991Z\",\"session_id\":\"72fc3f29-5d1e-4a24-8838-9446385e24bb\",\"session_start_time\":null,\"spark_pool\":null,\"state\":\"cancelled\",\"statement_id\":-1,\"statement_ids\":null}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d2fbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cod Concelho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21d57b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "    from io import BytesIO\n",
    "    import csv\n",
    "    import pandas as pd\n",
    "    from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e23f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark session\n",
    "    spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0531a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nome interno do dataset e local no Lakehouse\n",
    "    DATASET_URL = \"https://public.opendatasoft.com/api/explore/v2.1/catalog/datasets/georef-portugal-concelho/exports/csv\"\n",
    "    ABFS_DIR = \"abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/cod_concelho\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e49e8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dataset():\n",
    "        print(f\"üîÑ Downloading CSV from OpenDataSoft...\")\n",
    "        response = requests.get(DATASET_URL)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"‚ùå Failed to download CSV: {response.status_code}\")\n",
    "        return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447ed98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_separator(sample_text):\n",
    "        try:\n",
    "            dialect = csv.Sniffer().sniff(sample_text)\n",
    "            return dialect.delimiter\n",
    "        except csv.Error:\n",
    "            print(\"‚ö†Ô∏è Could not detect delimiter. Falling back to ';'\")\n",
    "            return \";\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c55c927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_and_upload_to_abfs():\n",
    "        csv_bytes = download_dataset()\n",
    "        sample = csv_bytes.decode('utf-8')[:1000]\n",
    "        sep = detect_separator(sample)\n",
    "        print(f\"‚ÑπÔ∏è Detected separator: '{sep}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d686b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load with pandas\n",
    "        pdf = pd.read_csv(BytesIO(csv_bytes), sep=sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76242760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Spark DataFrame\n",
    "        df = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29f0a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Fabric Lakehouse\n",
    "        df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(ABFS_DIR)\n",
    "        print(f\"‚úÖ Saved to: {ABFS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f137424f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "    save_and_upload_to_abfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03497e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"execution_finish_time\":\"2025-06-28T13:23:32.8864257Z\",\"execution_start_time\":\"2025-06-28T13:23:21.8494358Z\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\",\"parent_msg_id\":\"a3c04ddf-9e0b-42c1-a9de-b6917430a059\",\"queued_time\":\"2025-06-28T13:23:10.8699381Z\",\"session_id\":\"95fbb34a-43d1-4451-9444-db4ce24a7ca4\",\"session_start_time\":\"2025-06-28T13:23:10.8710217Z\",\"spark_pool\":null,\"state\":\"finished\",\"statement_id\":3,\"statement_ids\":[3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b1cab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "üîÑ Downloading CSV from OpenDataSoft...\n",
    "    ‚ö†Ô∏è Could not detect delimiter. Falling back to ';'\n",
    "    ‚ÑπÔ∏è Detected separator: ';'\n",
    "    ‚úÖ Saved to: abfss://Workspace_Tese_EREDES@onelake.dfs.fabric.microsoft.com/Lakehouse_EREDES_Tese.Lakehouse/Files/bronze/cod_concelho"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
